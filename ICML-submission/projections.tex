\section{Projections}
\label{section:projections}

In this section, we denote by $C_n$ the class of \emph{projections} over the
domain $\X = \{0,1\}^n$. The class $C_n$ consists of $n$ functions $c_1, c_2,
\dots, c_n$ from $\{0,1\}^n$ to $\{0,1\}$. For any $i \in \{1,2,\dots,n\}$, for
any $x \in \{0,1\}^n$, the function $c_i$ is defined as $c_i((x[1], x[2], \dots,
x[n])) = x[i]$.

For any $\epsilon \in (0,\frac{1}{2})$ and $n \ge 2$, we consider a family
$\P_{n,\epsilon}$ consisting of $n$ probability distributions $P_1, P_2, \dots,
P_n$ over the Boolean hypercube $\{0,1\}^n$. In order to describe the
distribution $P_i$, for some $i$, consider a random vector $X = (X[1], X[2],
\dots, X[n])$ drawn from $P_i$. The distribution $P_i$ is a product
distribution, i.e., $\Pr[X = x] = \prod_{j=1}^n \Pr[X[j] = x[j]]$ for any $x \in
\{0,1\}^n$. The marginal distributions of the coordinates are
$$
\Pr[X[j] = 1] =
\begin{cases}
\frac{1}{2} & \text{if $j = i$,} \\
\epsilon & \text{if $j\neq i$,} \\
\end{cases}
\qquad \text{for $j=1,2,\dots,n$.}
$$
The reader should think of $\epsilon$ as a constant that does not depend on $n$,
say, $\epsilon=\frac{1}{100}$.

\begin{proposition}
\label{proposition:vc-dimension-projections}
Vapnik-Chervonenkis dimension of $C_n$ is $\lfloor \log_2 n \rfloor$.
\end{proposition}

\begin{proof}
Let us denote the Vapnik-Chervonenkis dimension by $d$. Recall that $d$ is the
size of the largest shattered set. Let $S$ be any shattered set of size $d$.
Then, there must be at least $2^d$ distinct functions in $C_n$. Hence, $d \le
\log_2 |C_n| = \log_2 n$. Since $d$ is an integer, we conclude that $d \le
\lfloor \log_2 n \rfloor$.

On the other hand, we construct a shattered set of size $\lfloor \log_2 n
\rfloor$. The set will consists of points $x_1, x_2, \dots, x_{\lfloor \log_2 n
\rfloor} \in \{0,1\}^n$. For any $i \in \{1,2,\dots,\lfloor \log_2 n \rfloor\}$
and any $j \in \{0,1,2,\dots,n-1\}$, we define $x_i[j]$ to be the $i$-th bit
in the binary representation of the number $j$. (The bit at position $i=1$ is the
least significant bit.) It is not hard to see that for any $v \in
\{0,1\}^{\lfloor \log_2 n \rfloor}$, there exists $c \in C_n$ such that $v =
(c(x_1), c(x_2), \dots, c(x_{\lfloor \log_2 n \rfloor}))$. Indeed, given $v$,
let $k \in \{0,1,\dots,2^{\lfloor \log_2 n \rfloor} - 1\}$ be the number with
binary representation $v$, then we can take $c = c_{k+1}$.
\end{proof}


\begin{lemma}[Small cover]
Let $n \ge 2$ and $\epsilon \in (0,\frac{1}{2})$. Any distribution in $\P_{n,\epsilon}$
has $2\epsilon$-cover of size $2$.
\end{lemma}

\begin{proof}
Consider a distribution $P_i \in \P_{n,\epsilon}$ for some $i \in \{1,2,\dots,n\}$.
Let $j$ be an arbitrary index in $\{1,2,\dots,n\} \setminus \{i\}$.
Consider the projections $c_i, c_j \in C_n$. We claim that $C' = \{c_i, c_j\}$
is a $2\epsilon$-cover of $C_n$.

To see that $C'$ is a $2\epsilon$-cover of $C_n$, consider any $c_k \in C_n$.
We need to show that $d_{P_i}(c_i, c_k) \le 2\epsilon$ or $d_{P_i}(c_j, c_k)
\le 2\epsilon$. If $k = i$ or $k = j$, the condition is trivially satisfied.
Consider $k \in \{1,2,\dots,n\} \setminus \{i,j\}$. Let $X \sim P_i$. Then,
\begin{align*}
d_{P_i}(c_j, c_k)
& = \Pr[c_j(X) \neq c_k(X)] \\
& = \Pr[c_j(X) = 1 \wedge c_k(X) = 0] \\
  & \qquad + \Pr[c_j(X) = 0 \wedge c_k(X) = 1] \\
& = \Pr[X[j] = 1 \wedge X[k] = 0]  \\
  & \qquad + \Pr[X[j] = 0 \wedge X[k] = 1] \\
& = \Pr[X[j] = 1] \Pr[X[k] = 0] \\
  & \qquad + \Pr[X[j] = 0] \Pr[X[k] = 1] \\
& = 2 \epsilon \left( 1 - \epsilon \right)  \\
& < 2 \epsilon \; .
\end{align*}
\end{proof}

Using Benedek-Itai bound (see the full version of the paper) we obtain the corollary below.
The corollary states that the distribution-dependent sample complexity
of learning target in $C_n$ under any distribution from $P_{n,\epsilon}$
does \emph{not} depend on $n$.

\begin{corollary}[Learning with knowledge of the distribution]
Let $n \ge 2$ and $\epsilon \in (0,\frac{1}{2})$.  There exists a
distribution-dependent algorithm such that for any distribution from $\P_{n,\epsilon}$,
any $\delta \in (0,1)$, any target function $c \in C_n$, if the algorithm gets
$$
m \ge \frac{12\ln(2/\delta)}{\epsilon}
$$
labeled examples, with
probability at least $1 - \delta$, it $4\epsilon$-learns the target.
\end{corollary}

The next theorem states that without knowing the distribution,
learning a target under a distribution from $\P_{n,\epsilon}$
requires at least $\Omega(\log n)$ labeled examples.

\begin{theorem}[Learning without knowledge of the distribution]
For any distribution-independent algorithm, any $\epsilon \in (0,\frac{1}{4})$ and any
$n \ge 600/\epsilon^3$ there exists a distribution $P \in \P_{n,\epsilon}$ and a target
concept $c \in C_n$ such that if the algorithm gets
$$
m \le \frac{\ln n}{3 \ln (1/\epsilon)}
$$
labeled examples, it fails to $\frac{1}{16}$-learn the target concept with probability
more than $\frac{1}{16}$.
\end{theorem}

\begin{proof}
Let $A$ be any learning algorithm. For ease of notation, we formalize it is a function
$$
A:\bigcup_{m=0}^\infty \left(\{0,1\}^{m \times n} \times \{0,1\}^m\right) \to \{0,1\}^{\{0,1\}^n} \; .
$$
The algorithm receives an $m \times n$ matrix and a binary vector of length $m$.
The rows of the matrix corresponds to unlabeled examples and the vector encodes
the labels. The output of $A$ is any function from $\{0,1\}^n \to \{0,1\}$.

We demonstrate the existence of a pair $(P,c) \in \P_{n,\epsilon} \times C_n$ which
cannot be learned with $m$ samples by the probabilistic method. Let $I$ be chosen
uniformly at random from $\{1,2,\dots,n\}$. We consider the distribution $P_I \in
\P_{n,\epsilon}$ and target $c_I \in C_n$. Let $X_1, X_2, \dots, X_m$ be an i.i.d.
sample from $P_I$ and let $Y_1 = c_I(X_1), Y_2 = c_I(X_2), \dots, Y_m =
c_I(X_m)$ be the target labels. Let $X$ be the $m \times n$ matrix with entries
$X_i[j]$ and let $Y = (Y_1, Y_2, \dots, Y_m)$ be the vector of labels. The
output of the algorithm is $A(X,Y)$. We will show that
\begin{equation}
\label{equation:projections-failure-probability}
\Exp \left[d_{P_I}(c_I, A(X,Y)) \right] \ge \frac{1}{8} \; .
\end{equation}
This means that there exists $i \in \{1,2,\dots,n\}$ such that
$$
\Exp \left[d_{P_i}(c_i, A(X,Y)) ~\middle|~ I = i \right] \ge \frac{1}{8} \; .
$$
By Proposition~\ref{proposition:error-probability-vs-expected-error},
$$
\Pr \left[ d_{P_i}(c_i, A(X,Y)) > \frac{1}{16} ~\middle|~ I = i \right] \ge \frac{\frac{1}{8} - \frac{1}{16}}{1 - \frac{1}{16}} > \frac{1}{16} \; .
$$

It remains to prove \eqref{equation:projections-failure-probability}. Let $Z$ be
a test sample drawn from $P_I$. That is, conditioned on $I$, the sequence $X_1,
X_2, \dots, X_m, Z$ is i.i.d. drawn from $P_I$. Then, by
Proposition~\ref{proposition:single-bit},
\begin{multline}
\label{equation:projections-expected-error-lower-bound}
\Exp \left[d_{P_I}(c_I, A(X,Y))\right]
= \Pr\left[ A(X,Y)(Z) \neq c_I(Z) \right] \ge \\
\sum_{\substack{x \in \{0,1\}^{m \times n} \\ y \in \{0,1\}^m \\ z \in \{0,1\}^n}} \left( \frac{1}{2} - \left| \frac{1}{2} - \Exp\left[ c_I(Z) \, \middle| \, X = x, Y = y, Z = z \right] \right| \right)
\\ \cdot \Pr \left[X = x, Y = y, Z = z \right]  \; .
\end{multline}
We need to compute $\Exp\left[ c_I(Z) \, \middle| \, X = x, Y = y, Z = z \right]$.
For that we need some additional notation.
For any matrix $x \in \{0,1\}^{m \times n}$, let $x[1], x[2], \dots, x[n]$ be its columns.
For any matrix $x \in \{0,1\}^{m \times n}$ and vector $y \in \{0,1\}^m$ let
$$
k(x,y) = \{ i \in \{1,2,\dots,n\} ~:~ x[i] = y \}
$$
be the set of indices of columns of $x$ equal to the vector $y$. Also, we define
$\norm{\cdot}$ to be the sum of absolute values of entries of a vector or a
matrix. (Since we use $\norm{\cdot}$ only for binary matrices and binary
vectors, it will be just the number of ones.)

For any $i \in \{1,2,\dots,n\}$,
\begin{multline*}
\Pr \left[I = i, X = x, Y = y \right] \\
=
\begin{cases}
\frac{1}{n} \left( \frac{1}{2} \right)^m \epsilon^{\norm{x} - \norm{y}} (1 - \epsilon)^{mn - \norm{x} + \norm{y}} & \text{if $i \in k(x,y)$,} \\
0 & \text{if $i \not \in k(x,y)$.} \\
\end{cases}
\end{multline*}
Therefore, for any $i \in \{1,2,\dots,n\}$,
\begin{align*}
& \Pr \left[I = i \, \middle| \, X = x, Y = y \right] \\
& \qquad = \frac{\Pr \left[I = i, X = x, Y = y \right]}{\Pr \left[ X = x, Y = y \right]} \\
& \qquad = \frac{\Pr \left[I = i, X = x, Y = y \right]}{\sum_{j \in k(x,y)} \Pr \left[ I = j, X = x, Y = y \right]} \\
& \qquad =
\begin{cases}
\frac{1}{|k(x,y)|} & \text{if $i \in k(x,y)$,} \\
0 & \text{if $i \not \in k(x,y)$.} \\
\end{cases}
\end{align*}

Conditioned on $I$, the variables $Z$ and $(X,Y)$ are independent. Thus,
for any $x \in \{0,1\}^n$, and $i = 1,2,\dots,n$,
\begin{align*}
& \Pr \left[Z = z \, \middle| \, I = i, X = x, Y = y \right] \\
& \qquad = \Pr \left[Z = z \, \middle| \, I = i \right] \\
& \qquad =
\begin{cases}
\frac{1}{2} \epsilon^{\norm{z} - 1} (1 - \epsilon)^{n - \norm{z}} & \text{if $z[i] = 1$,} \\
\frac{1}{2} \epsilon^{\norm{z}} (1 - \epsilon)^{n-1 - \norm{z}}& \text{if $z[i] = 0$.} \\
\end{cases}
\end{align*}
This allows us to compute the conditional probability
\begin{align*}
& \Pr \left[I = i, Z = z \, \middle| \, X = x, Y = y \right] \\
& \quad = \Pr \left[Z = z \, \middle| \, I = i, X = x, Y = y \right] \cdot \Pr \left[I = i \, \middle| \, X = x, Y = y \right] \\
& \quad =
\begin{cases}
\frac{1}{2|k(x,y)|} \epsilon^{\norm{z} - 1} (1 - \epsilon)^{n - \norm{z}} & \text{if $i \in k(x,y)$ and $z[i] = 1$,} \\
\frac{1}{2|k(x,y)|} \epsilon^{\norm{z}} (1 - \epsilon)^{n - 1 - \norm{z}} & \text{if $i \in k(x,y)$ and $z[i] = 0$,} \\
0 & \text{if $i \not \in k(x,y)$.} \\
\end{cases}
\end{align*}
For any $z \in \{0,1\}^n$, let
$$
s(x,y,z) = \{ i \in k(x,y) ~:~ z[i] = 1 \} \; ,
$$
and note that $s(x,y,z) \subseteq k(x,y)$.
Then,
\begin{align*}
& \Pr \left[Z = z \, \middle| \, X = x, Y = y \right] \\
& \quad = \sum_{i=1}^n \Pr \left[Z = z, I = i \, \middle| \, X = x, Y = y \right] \\
& \quad = \sum_{i \in k(x,y)} \Pr \left[Z = z, I = i \, \middle| \, X = x, Y = y \right] \\
& \quad = \sum_{i \in s(x,y,x)} \Pr \left[Z = z, I = i \, \middle| \, X = x, Y = y \right] \\
& \qquad + \sum_{i \in k(x,y) \setminus s(x,y,z)} \Pr \left[Z = z, I = i \, \middle| \, X = x, Y = y \right] \\
& \quad = \frac{1}{2|k(x,y)|} \cdot |s(x,y,z)| \cdot \epsilon^{\norm{z} - 1} (1 - \epsilon)^{n - \norm{z}} \\
& \qquad + \frac{1}{2|k(x,y)|} \cdot (|k(x,y)| - |s(x,y,z)|) \cdot \epsilon^{\norm{z}} (1 - \epsilon)^{n - 1 - \norm{z}} \\
& \quad = \frac{\epsilon^{\norm{z} - 1} (1 - \epsilon)^{n - 1 - \norm{z}}}{2|k(x,y)|} \cdot \left( |s(x,y,z)| \cdot (1 - 2\epsilon) + |k(x,y)| \cdot \epsilon \right) \; .
\end{align*}
Hence,
\begin{align*}
& \Exp\left[ c_I(Z) \, \middle| \, X = x, Y = y, Z = z \right] \\
& \quad = \Pr \left[ Z[I] = 1 \, \middle| \, X = x, Y = y, Z = z \right] \\
& \quad = \frac{\displaystyle \Pr \left[ Z[I] = 1, Z = z \, \middle| \, X = x, Y = y \right]}{\displaystyle \Pr \left[ Z = z \, \middle| \, X = x, Y = y \right]} \\
& \quad = \frac{\displaystyle \sum_{i=1}^n \Pr \left[ I = i, Z[i] = 1, Z = z \, \middle| \, X = x, Y = y \right]}{\displaystyle \Pr \left[ Z = z \, \middle| \, X = x, Y = y \right]} \\
& \quad = \frac{\displaystyle \frac{|s(x,y,z)|}{2|k(x,y)|} \cdot \epsilon^{\norm{z} - 1} (1 - \epsilon)^{n - \norm{z}}}{\displaystyle \frac{\epsilon^{\norm{z} - 1} (1 - \epsilon)^{n - 1 - \norm{z}}}{2|k(x,y)|}} \\
& \qquad \cdot \frac{1}{\left( |s(x,y,z)| \cdot (1 - 2\epsilon) + |k(x,y,z)| \cdot \epsilon \right) } \\
& \quad = \frac{\displaystyle |s(x,y,z)| \cdot (1 - \epsilon)}{\displaystyle |s(x,y,z)| \cdot (1 - 2\epsilon) + |k(x,y)| \cdot \epsilon } \\
& \quad = \frac{\displaystyle 1 - \epsilon}{\displaystyle 1 - 2\epsilon + \frac{|k(x,y)| \cdot \epsilon}{|s(x,y,z)|}} \\
\end{align*}

We now show that the last expression is close to $1/2$. It is easy to check that
$$
\frac{|k(x,y)| \cdot \epsilon}{|s(x,y,z)|} \in \left[\frac{5}{6}, 2 \right] \ \Rightarrow \ \frac{\displaystyle 1 - \epsilon}{\displaystyle 1 - 2\epsilon + \frac{|k(x,y)| \cdot \epsilon}{|s(x,y,z)|}} \in \left[ \frac{1}{4}, \frac{3}{4} \right].
$$
Indeed, since $\epsilon \in (0,\frac{1}{4})$,
$$
\frac{\displaystyle 1 - \epsilon}{\displaystyle 1 - 2\epsilon + \frac{|k(x,y)| \cdot \epsilon}{|s(x,y,z)|}} \ge
\frac{\displaystyle 1 - \epsilon}{\displaystyle 1 - 2\epsilon + 2} \ge \frac{\displaystyle 1 - 1/4}{\displaystyle 1 + 2} = \frac{1}{4}
$$
and
\begin{align*}
\frac{\displaystyle 1 - \epsilon}{\displaystyle 1 - 2\epsilon + \frac{|k(x,y)| \cdot \epsilon}{|s(x,y,z)|}}
& \le \frac{\displaystyle 1 - \epsilon}{\displaystyle 1 - 2\epsilon + 5/6} \\
& \le \frac{\displaystyle 1}{\displaystyle 1 - 1/2 + 5/6} = \frac{3}{4} \; .
\end{align*}
We now substitute this into the \eqref{equation:projections-expected-error-lower-bound}. We have
\begin{align*}
& \sum_{\substack{x \in \{0,1\}^{m \times n} \\ y \in \{0,1\}^m \\ z \in \{0,1\}^n}} \left( \frac{1}{2} - \left| \frac{1}{2} - \Exp\left[ c_I(Z) \, \middle| \, X = x, Y = y, Z = z \right] \right| \right) \\
& \qquad \qquad \qquad  \cdot \Pr \left[X = x, Y = y, Z = z \right] \\
& = \sum_{\substack{x \in \{0,1\}^{m \times n} \\ y \in \{0,1\}^m \\ z \in \{0,1\}^n}} \left( \frac{1}{2} - \left| \frac{1}{2} - \frac{\displaystyle 1 - \epsilon}{\displaystyle 1 - 2\epsilon + \frac{|k(x,y)| \cdot \epsilon}{|s(x,y,z)|}} \right| \right) \\
& \qquad \qquad \qquad  \cdot \Pr \left[X = x, Y = y, Z = z \right] \\
& \ge
\sum_{\substack{x \in \{0,1\}^{m \times n} \\ y \in \{0,1\}^m \\ z \in \{0,1\}^n \\ \frac{|k(x,y,z)| \epsilon}{|s(x,y,z)|} \in [\frac{5}{6},2]}} \left( \frac{1}{2} - \left| \frac{1}{2} - \frac{\displaystyle 1 - \epsilon}{\displaystyle 1 - 2\epsilon + \frac{|k(x,y)| \cdot \epsilon}{|s(x,y,z)|}} \right|  \right) \\
& \qquad \qquad \qquad  \cdot \Pr \left[X = x, Y = y, Z = z \right] \\
& \ge
\sum_{\substack{x \in \{0,1\}^{m \times n} \\ y \in \{0,1\}^m \\ z \in \{0,1\}^n \\ \frac{|k(x,y,z)| \epsilon}{|s(x,y,z)|} \in [\frac{5}{6},2]}} \left( \frac{1}{2} - \frac{1}{4} \right) \cdot \Pr \left[X = x, Y = y, Z = z \right] \\
& =
\frac{1}{4} \Pr \left[ \frac{|k(X,Y)| \cdot \epsilon}{|s(X,Y,Z)|} \in \left[\frac{5}{6}, 2 \right]  \right] \; .
\end{align*}
In order to prove \eqref{equation:projections-failure-probability}, we need to show that
$\frac{|k(X,Y)| \cdot \epsilon}{|s(X,Y,Z)|} \in \left[\frac{5}{6}, 2 \right]$ with
probability at least $1/2$. To that end, we define two additional random
variables
$$
K = |k(X,Y)| \qquad \text{and} \qquad S = |s(X,Y,Z)| \; .
$$
The condition $\frac{|k(X,Y)| \cdot \epsilon}{|s(X,Y,Z)|} \in \left[\frac{5}{6}, 2 \right]$ is equivalent to
\begin{equation}
\label{equation:ratio-condition}
\frac{1}{2} \epsilon \le \frac{S}{K} \le \frac{6}{5} \epsilon \; .
\end{equation}

First, we lower bound $K$. For any $y \in \{0,1\}^m$ and any $i,j \in \{1,2,\dots,n\}$,
\begin{align*}
& \Pr \left[ j \in k(X,Y)  \, \middle| \, Y = y, I = i \right] \\
& \qquad =
\begin{cases}
1 & \text{if $j = i$,} \\
\epsilon^{\norm{y}} (1 - \epsilon)^{m - \norm{y}} & \text{if $j \neq j$.}
\end{cases}
\end{align*}
Conditioned on $Y = y$ and $I=i$, the random variable $K - 1 = |k(X,Y) \setminus \{I\}|$ is
a sum of $n-1$ Bernoulli variables with parameter $\epsilon^{\norm{y}} (1 - \epsilon)^{m - \norm{y}}$, one for each column except for column $i$.
Hoeffding bound with $t = \epsilon^m/2$ and the loose lower bound $\epsilon^{\norm{y}} (1 - \epsilon)^{m - \norm{y}} \ge \epsilon^m$ gives
\begin{align*}
& \Pr \left[ \frac{K - 1}{n - 1} > \frac{\epsilon^m}{2}  \, \middle| \,  Y = y, I = i  \right] \\
& = \Pr \left[ \frac{K - 1}{n - 1} > \epsilon^m - t  \, \middle| \,  Y = y, I = i  \right] \\
& \ge \Pr \left[ \frac{K - 1}{n - 1} > \epsilon^{\norm{y}} (1 - \epsilon)^{m - \norm{y}} - t  \, \middle| \,  Y = y, I = i  \right] \\
& \ge 1 - e^{-2(n-1) t^2} \; .
\end{align*}
Since $m \le \frac{\ln n}{3 \ln (1/\epsilon)}$, we lower bound $t = \frac{\epsilon^m}{2}$ as
$$
t = \epsilon^m/2 > \frac{1}{2} \epsilon^\frac{\ln n}{3 \ln(1/\epsilon)} = \frac{1}{2\sqrt[3]{n}} \; .
$$
Since the lower bound is uniform for all choices of $y$ and $i$, we can remove
the conditioning and conclude that
$$
\Pr \left[ K > 1 + \frac{(n-1)}{2\sqrt[3]{n}} \right] \ge 1 - \exp \left(- \frac{(n-1)}{2 n^{2/3}} \right) \; .
$$
For $n \ge 25$, we can simplify it further to
$$
\Pr \left[ K \ge \frac{n^{2/3}}{2} \right] \ge \frac{3}{4} \; .
$$

Second, conditioned on $K=r$, the random variable $S$
is a sum of $r-1$ Bernoulli random variables with parameter $\epsilon$
and one Bernoulli random variable with parameter $1/2$. Hoeffding bound for any $t \ge 0$
gives that
\begin{align*}
\Pr \left[ \left| \frac{S}{K} - \frac{\epsilon(K - 1) + 1/2}{K} \right| < t \, \middle| \, K = r \right] \ge 1 - 2 e^{-2 r t^2} \; .
\end{align*}
Thus,
\begin{align*}
& \Pr \left[ \left| \frac{S}{K} - \frac{\epsilon(K - 1) + 1/2}{K} \right| < t \ \text{and} \ K \ge \frac{n^{2/3}}{2} \right] \\
& \ge \sum_{r = \lceil n^{2/3} / 2 \rceil}^n \Pr \left[ \left| \frac{S}{K} - \frac{\epsilon(K - 1) + 1/2}{K} \right| < t  \, \middle| \,  K = r \right] \\
& \qquad \qquad \qquad \qquad \cdot \Pr[K = r] \\
& \ge \sum_{r = \lceil n^{2/3} / 2 \rceil}^n \left( 1 - 2 e^{-2 r t^2} \right) \cdot \Pr[K = r] \\
& \ge \left( 1 - 2 e^{-n^{2/3}  t^2 / 2} \right) \cdot \Pr \left[ K \ge \frac{n^{2/3}}{2} \right] \; .
\end{align*}
We choose $t = \epsilon/4$. Since $n \ge 600/\epsilon^3$, we have $e^{-n^{2/3}  t^2 / 2} < \frac{1}{8}$ and thus
\begin{align*}
& \Pr \left[ \left| \frac{S}{K} - \frac{\epsilon(K - 1) + 1/2}{K} \right| < t \ \text{and} \ K \ge \frac{n^{2/3}}{2} \right] \\
& \qquad \ge \left( 1 - 2 e^{-n^{2/3}  t^2 / 2} \right) \cdot \Pr \left[ K \ge \frac{n^{2/3}}{2} \right] \\
& \qquad \ge \frac{3}{4} \left( 1 - 2 e^{-n^{2/3}  t^2 / 2} \right) \\
& \qquad > \frac{3}{4} \left( 1 - \frac{1}{4} \right) = \frac{9}{16} > \frac{1}{2} \; .
\end{align*}
We claim that $t = \epsilon/4$,
$\left| \frac{S}{K} - \frac{\epsilon(K - 1) + 1/2}{K} \right| < t$
and $K \ge \frac{n^{2/3}}{2}$ imply \eqref{equation:ratio-condition}.
To see that, note that $\left| \frac{S}{K} - \frac{\epsilon(K - 1) + 1/2}{K} \right| < t$ is equivalent to
$$
\frac{\epsilon(K - 1) + 1/2}{K} - t < \frac{S}{K} < \frac{\epsilon(K - 1) + 1/2}{K} + t
$$
which implies that
$$
p \left(1 - \frac{1}{K} \right) - t < \frac{S}{K} < \epsilon \left(1 - \frac{1}{K} \right) + \frac{1}{2K} + t \; .
$$
Since $K \ge \frac{n^{2/3}}{2}$ and $n \ge 25$ we have $K > 4$, which implies that
$$
\frac{3}{4} \epsilon - t < \frac{S}{K} < \frac{3}{4} \epsilon + \frac{1}{2K} + t \; .
$$
Since $K \ge \frac{n^{2/3}}{2}$ and $n \ge \frac{12}{\epsilon^{3/2}}$ we have $K > \frac{5}{2\epsilon}$, which implies that
$$
\frac{3}{4} \epsilon - t < \frac{S}{K} < \frac{3}{4} \epsilon + \frac{\epsilon}{5} + t \; .
$$
Since $t = \epsilon/4$, the condition \eqref{equation:ratio-condition} follows.
\end{proof}
