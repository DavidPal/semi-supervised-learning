\section*{Conclusions}
\citet{Darnstadt-Simon-Szorenyi-2013} showed that the gap between the number of samples needed to learn a class of functions of Vapnik-Chervonenkis dimension $d$ \emph{with} and \emph{without} knowledge of the distribution is upper-bounded by $O(d)$. We show that this bound is tight for the class of Boolean projections. On the other hand, for the class of all functions, this gap is only constant. These observations lead to the following research direction. 

First, it will be interesting to understand the value of the gap for larger classes of functions. For example, one might consider the classes of Boolean monotone disjunctions and halfspaces. We note that techniques similar to the ones used in this paper lead to a logarithmic lower bound on the gap for those classes, while their Vapnik-Chervonenkis dimensions are linear in the number of dimensions. 

Second, since we now know that the gap is not characterized by the Vapnik-Chervonenkis dimension, it will be interesting to study other parameters which determine this gap. In particular, it will be interesting to obtain other upper bounds on the gap.

Finally, we believe that studying this question in the agnostic case will be of great interest, too.
