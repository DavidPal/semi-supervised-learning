\section{Conclusion and open problems}
\label{section:conclusions}

\citet{Darnstadt-Simon-Szorenyi-2013} showed that the gap between the number of
samples needed to learn a class of functions of Vapnik-Chervonenkis dimension
$d$ \emph{with} and \emph{without} knowledge of the distribution is
upper-bounded by $O(d)$. We show that this bound is tight for the class of
Boolean projections. On the other hand, for the class of all functions, this gap
is only constant. These observations lead to the following research directions.

First, it will be interesting to understand the value of the gap for larger
classes of functions. For example, one might consider the classes of (monotone)
disjunctions over $\{0,1\}^n$, (monotone) conjuctions over $\{0,1\}^n$, parities
over $\{0,1\}^n$, and halfspaces over $\R^n$. The Vapnik-Chervonenkis dimension
of these classes is $\Theta(n)$ thus the gap for these classes is at least
$\Omega(1)$ and at most $O(n)$. Other than these crude bounds, the question of
what is the gap for these classes is wide open.

Second, as the example with class of all functions shows, the gap is \emph{not}
characterized by the Vapnik-Chervonenkis dimension. It will be interesting to
study other parameters which determine this gap. In particular, it will be
interesting to obtain upper bounds on the gap in terms of other quantities.

Finally, we believe that studying this question in the agnostic extension of the
PAC model \citep[Chapter~2]{Anthony-Bartlett-1999} will be of great interest,
too.
