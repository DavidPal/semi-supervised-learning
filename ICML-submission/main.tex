\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{subfigure}
\usepackage{todonotes}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}
%\usepackage{icml2019}
\usepackage[accepted]{icml2019}

% autoref fix for Chapter
\addto\extrasenglish{%
  \renewcommand{\sectionautorefname}{Section}%
}

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}

\newcommand{\R}{\mathbb{R}}

\renewcommand{\P}{\mathcal{P}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\indicator}[1]{\mathbf{1}\left[{#1}\right]}
\newcommand{\x}{\mathbf{x}}
\newcommand{\KL}[2]{D\left(#1 \middle\| #2 \right)}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\newcommand{\ip}[2]{\left\langle #1, #2 \right\rangle}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\Exp}{\mathbf{E}}
\DeclareMathOperator{\VC}{VC}

\newcommand{\todoBalazs}[1]{\todo[backgroundcolor=blue]{Balazs: #1}}
\newcommand{\todoDavid}[1]{\todo[backgroundcolor=green]{David: #1}}
\newcommand{\todoSasha}[1]{\todo[backgroundcolor=orange]{Sasha: #1}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{The information-theoretic value of unlabeled data in semi-supervised learning}

\begin{document}

\twocolumn[
\icmltitle{The information-theoretic value of unlabeled data in semi-supervised learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2019
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Alexander Golovnev}{harvard}
\icmlauthor{D\'avid P\'al}{yahoo}
\icmlauthor{Bal\'azs Sz\"or\'enyi}{yahoo}
\end{icmlauthorlist}

\icmlaffiliation{harvard}{Harvard University, Cambridge, MA, USA}
\icmlaffiliation{yahoo}{Yahoo Research, New York, NY, USA}

\icmlcorrespondingauthor{D\'avid P\'al}{davidko.pal@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{semi-supervised learning, PAC learning, sample complexity, unlabeled data}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We show a separation of the number of labeled examples required to learn between
two settings: Settings \emph{with} and \emph{without} the knowledge of the
distribution of the unlabeled data. For the class of projections over the
Boolean hypercube of dimension $n$, we show a separation by $\Theta(\log n)$
multiplicative factor. For the class of monotone disjunctions over the Boolean
hypercube of dimension $n$, we show a separation by $\Theta(n)$ multiplicative
factor. For the class of halfspaces over $\R^n$, we show a separation by
$\Theta(n/\log n)$ multiplicative factor.

Learning with the knowledge of the distribution (a.k.a. \emph{fixed-distribution
learning}) can be viewed as an idealized scenario of semi-supervised learning
where the number of unlabeled data points is so great that the unlabeled
distribution is known exactly. For this reason, we call the separation the
\emph{value of unlabeled data}.
\end{abstract}

\input{introduction}

\bibliography{biblio}
\bibliographystyle{icml2019}

\appendix

\end{document}
