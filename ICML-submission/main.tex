\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{subfigure}
\usepackage{todonotes}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}
%\usepackage{icml2019}
\usepackage[accepted]{icml2019}

% autoref fix for Chapter
\addto\extrasenglish{%
  \renewcommand{\sectionautorefname}{Section}%
}

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}

\newcommand{\R}{\mathbb{R}}

\renewcommand{\P}{\mathcal{P}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\indicator}[1]{\mathbf{1}\left[{#1}\right]}
\newcommand{\x}{\mathbf{x}}
\newcommand{\KL}[2]{D\left(#1 \middle\| #2 \right)}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\newcommand{\ip}[2]{\left\langle #1, #2 \right\rangle}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\Exp}{\mathbf{E}}
\DeclareMathOperator{\VC}{VC}

\newcommand{\todoBalazs}[1]{\todo[backgroundcolor=blue]{Balazs: #1}}
\newcommand{\todoDavid}[1]{\todo[backgroundcolor=green]{David: #1}}
\newcommand{\todoSasha}[1]{\todo[backgroundcolor=orange]{Sasha: #1}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{The information-theoretic value of unlabeled data in semi-supervised learning}

\begin{document}

\twocolumn[
\icmltitle{The information-theoretic value of unlabeled data in semi-supervised learning}

%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Alexander Golovnev}{harvard}
\icmlauthor{D\'avid P\'al}{yahoo}
\icmlauthor{Bal\'azs Sz\"or\'enyi}{yahoo}
\end{icmlauthorlist}

\icmlaffiliation{harvard}{Harvard University, Cambridge, MA, USA}
\icmlaffiliation{yahoo}{Yahoo Research, New York, NY, USA}

\icmlcorrespondingauthor{D\'avid P\'al}{davidko.pal@gmail.com}

\icmlkeywords{semi-supervised learning, PAC learning, sample complexity, unlabeled data}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}
We quantify the separation between the numbers of labeled examples required to
learn in two settings: Settings \emph{with} and \emph{without} the knowledge of
the distribution of the unlabeled data. More specifically, we prove a separation
by $\Theta(\log n)$ multiplicative factor for that the class of projections over
the Boolean hypercube of dimension $n$.

Learning with the knowledge of the distribution (a.k.a. \emph{fixed-distribution
learning}) can be viewed as an idealized scenario of semi-supervised learning
where the number of unlabeled data points is so great that the unlabeled
distribution is known exactly. For this reason, we call the separation the
\emph{value of unlabeled data}.
\end{abstract}

\input{introduction}
\input{related-work}
\input{preliminaries}

\bibliography{biblio}
\bibliographystyle{icml2019}

\appendix

\end{document}
