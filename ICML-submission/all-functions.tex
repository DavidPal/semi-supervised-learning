\section{All functions}
\label{section:all-functions}

Let $\X$ be some finite domain. We say a sample $T = ((x_1,y_1),
\dots,(x_m,y_m)) \in (\X \times \{0,1\})^m$ of size $m$ is
\emph{self-consistent} if for any $i,j \in \{1,2,\dots,m\}$, $x_i = x_j$
implies that $y_i = y_j$. A distribution independent algorithm $A$ is said to be
\emph{consistent} if for any self-consistent sample $T = ((x_1,y_1),
\dots,(x_m,y_m)) \in (\X \times \{0,1\})^m$, $A(T)(x_i) = y_i$ holds for any
$i=1,2,\dots,m$.

In this section we show that for $C_{\text{all}} = \{0,1\}^\X$, any consistent
distribution independent learner is almost as powerful as any distribution
independent learner. Note that, in particular, the ERM algorithm for
$C_{\text{all}}$ is consistent. In other words, for the class $C_{\text{all}}$
unlabeled data do \emph{not} have any information theoretic value.

\begin{theorem}[No Gap]
Let $\X$ be some finite domain, $C_{\text{all}} = \{0,1\}^\X$ and $A$ be any
consistent learning algorithm. Then, for any distribution $P$ over $\X$, any
(possibly distribution dependent) learning algorithm $B$ and any $\epsilon, \delta \in (0,1)$,
$$
m(A,C_{\text{all}},P,2\epsilon,2\delta) \le m(B,C_{\text{all}},P,\epsilon,\delta) \; .
$$
\end{theorem}

\begin{proof}
Fix any integer $m \ge 0$ and any distribution $P$ over $\X$. Let $X, X_1, X_2,
\dots, X_m$ be an i.i.d. sample from $P$.
Define the random variable
$$
Z = \Pr[X \not \in \{X_1, X_2, \dots, X_m\} ~|~ X_1, X_2, \dots, X_m] \; .
$$
In other words, $Z$ is the probability mass not covered by $X_1, X_2, \dots, X_m$.
For any $c \in C_{\text{all}}$, let
$T_c = ((X_1, c(X_1)), (X_2, c(X_2), \dots, (X_m, c(X_m))$ be the sample labeled
according to $c$. Since $A$ is consistent, with probability one,
for any $c \in C_{\text{all}}$,
\begin{equation}
\label{equation:relate-dP-to-Z}
d_P(A(T_c),c) \leq Z \;.
\end{equation}
Let $\widetilde c$ be chosen uniformly at random from
$C_{\text{all}}$, independently of $X, X_1, X_2, \dots, X_m$. We have
\begin{align}
& \sup_{c \in C_{\text{all}}} \Pr[d_P(B(T_c),c) \ge \epsilon] \notag \\
& = \sup_{c \in C_{\text{all}}} \Exp \left[ \indicator{d_P \left(B\left( T_c\right), c \right) \ge \epsilon} \right] \notag \\
& \ge \Exp \left[ \indicator{d_P \left(B\left( T_{\widetilde c} \right), \widetilde c \right) \ge \epsilon} \right] \notag \\
& = \Exp \left[ \Exp \left[ \indicator{d_P \left(B\left( T_{\widetilde c} \right), \widetilde c \right) \ge \epsilon} ~\middle|~ T_{\widetilde{c}} \right] \right] \notag \\
& \ge \Exp \left[ \Exp \left[ \frac{1}{2} \indicator{Z \ge 2 \epsilon} ~\middle|~ T_{\widetilde{c}} \right] \right] \label{equation:switch-to-Z} \\
& = \frac{1}{2} \Exp \left[ \indicator{Z \ge 2 \epsilon} \right] \notag \\
& = \frac{1}{2} \Pr \left[ Z \ge 2 \epsilon \right] \notag \\
& = \frac{1}{2} \sup_{c \in C} \Pr \left[ Z \ge 2 \epsilon \right] \notag \\
& \ge \frac{1}{2} \sup_{c \in C} \Pr \left[ d_P(A(T_c), c) \ge 2 \epsilon \right] \label{equation:use-dP-to-Z} \; .
\end{align}
To justify inequality \eqref{equation:switch-to-Z}, consider a fixed
$T_{\widetilde{c}}$. The classifier $B(T_{\widetilde{c}})$ is fixed and thus the
distance $d_P(B(T_{\widetilde{c}}), \widetilde{c})$ depends only on
$\widetilde{c}$ and, in particular, it depends only on the values
that $\widetilde{c}$ assigns to points in $\X \setminus \{X_1, X_2,
\dots, X_n\}$. The values of $c(x)$ for $x \in \X \setminus \{X_1, X_2, \dots, X_n\}$ are i.i.d. Bernoulli random variables
with parameter $1/2$. Define $\widehat c \in C_{\text{all}}$ as
$$
\widehat c(x) =
\begin{cases}
\widetilde{c}(x) & \text{if $x \in \{X_1, X_2, \dots, X_m\}$,} \\
1 - \widetilde{c}(x) & \text{otherwise}.
\end{cases}
$$
and note that $T_{\widetilde{c}} = T_{\widehat{c}}$.
Since the classifiers $\widetilde{c}$ and $\widehat{c}$ disagree on the missing mass,
if $Z \ge 2\epsilon$ then $d_P(B(T_{\widetilde{c}}), \widetilde{c})
\ge \epsilon$ or $d_P(B(T_{\widehat c}), \widehat c) \ge \epsilon$ or both.
By symmetry between $\widetilde{c}$ and $\widehat{c}$, if $Z \ge 2\epsilon$
then with probability at least $1/2$, $d_P(B(T_{\widetilde{c}}), \widetilde{c})
\ge \epsilon$. Inequality \eqref{equation:use-dP-to-Z} follows from
\eqref{equation:relate-dP-to-Z}.

Since the inequality
$$
\sup_{c \in C_{\text{all}}} \Pr[d_P(B(T_c),c) \ge \epsilon] \ge \frac{1}{2} \sup_{c \in C} \Pr \left[ d_P(A(T_c), c) \ge 2 \epsilon \right]
$$
holds for arbitrary $m$, it
implies $m(A,C_{\text{all}},P,2\epsilon,2\delta) \le m(B,C_{\text{all}},P,\epsilon,\delta)$ for any $\epsilon, \delta \in (0,1)$.
\end{proof}
