\section{Fixed distribution learning}
\label{section:fixed-distribution-learning}

\begin{theorem}[Chernoff bound]
Let $X_1, X_2, \dots, X_n$ be i.i.d. Bernoulli random variables with $\Exp[X_i] = p$.
Then, for any $\epsilon \in [0, \min\{p,1-p\})$,
$$
\Pr \left[{\frac {1}{n}} \sum_{i=1}^n X_i \ge p + \epsilon \right] \le e^{ - n \KL{p + \epsilon}{p}}  \qquad \text{and} \qquad
\Pr \left[{\frac {1}{n}} \sum_{i=1}^n X_i \le p - \epsilon \right] \le e^{ - n \KL{p - \epsilon}{p}}  \; . \\
$$
where
$$
\KL{x}{y} = x \ln \left( \frac{x}{y} \right) + (1 - x) \ln \left( \frac{1-x}{1-y} \right)
$$
is the Kullback-Leibler divergence between Bernoulli distributions with parameters $x, y \in [0,1]$.
\end{theorem}

We further use the following inequality
$$
\KL{x}{y} \ge \frac{(x-y)^2}{2 \max\{x, y\}}
$$


\begin{theorem}[Benedek-Itai]
Let $C \subseteq \{0,1\}^\X$ be a concept class over a non-empty domain $\X$.
Let $P$ be a distribution over $\X$. Let $\epsilon \in (0,1]$ and assume that
$C$ has an $(\epsilon/2)$-cover of size at most $N$. Then, there exists an algorithm, such
that for any $\delta \in (0,1)$, any target $c \in C$, if it gets
$$
m \ge 48\left(\frac{\ln N + \ln(1/\delta)}{\epsilon}\right)
$$
labeled samples then with probability at least $1 - \delta$, it
$\epsilon$-learns the target.
\end{theorem}

\begin{proof}
Given a labeled sample $T = (x_1, y_1), \dots, (x_m, y_m)$, for any $c \in C$,
we define
$$
\err_T(c) = \frac{1}{m} \sum_{i=1}^m \indicator{c(x_i) \neq y_i} \; .
$$

Let $C' \subseteq C$ be an $(\epsilon/2)$-cover of size at most $N$.
Condiser the algorithm $A$ that given a labeled sample $T$ outputs
$$
\widehat c = \argmin_{c' \in C'} \err_T(c')
$$
breaking ties arbitrarily. We prove that $A$, with probability at least
$1-\delta$, $\epsilon$-learns any target $c \in C$ under the distribution $P$.

Consider any target $c \in C$. Then there exists $\widetilde c \in C'$ such that
$d_P(c,\widetilde c) \le \epsilon/2$. Let $C'' = \{ c' ~:~ d_P(c,c') > \epsilon \}$.
We claim that with probability at least $1 - \delta$, for all $c' \in C''$,
$\err_T(c') > \frac{2}{3} \epsilon$ and $\err_T(\widetilde{c}) <
\frac{2}{3}\epsilon$ and hence $A$ outputs $\widehat c \in C' \setminus C''$.

Consider any $c' \in C''$ and note that $\err_T(c')$ is an average of Bernoulli
random variables with mean $d_P(c,c') > \epsilon$. Thus, by Chernoff bound,
\begin{align*}
\Pr \left[ \err_T(c') > \frac{2}{3} \epsilon \right]
& > 1 - \exp \left( - m \KL{\frac{2}{3} \epsilon}{d_P(c,c')} \right) \\
& > 1 - \exp \left( - m \frac{(\frac{2}{3} \epsilon - d_P(c,c'))^2}{2 d_P(c,c')} \right) \\
& > 1 - \exp \left( - m \epsilon/18 \right)
\end{align*}
where the last inequality follows from the inequality
$$
\left( \frac{2}{3} \epsilon - x \right)^2 \ge \frac{1}{9} \epsilon x
$$
valid for any $x \ge \epsilon > 0$.
Similarly, $\err_T(\widetilde{c})$ is an average of Bernoulli random variables with mean $d_P(c, \widetilde{c}) < \epsilon/2$.
Thus, by Chernoff bound,
\begin{align*}
\Pr \left[ \err_T(\widetilde{c}) < \frac{2}{3} \epsilon \right]
& > 1 - \exp \left( - m \KL{\frac{2}{3} \epsilon}{d_P(c, \widetilde{c})} \right) \\
& > 1 - \exp \left( - m \frac{(\frac{2}{3} \epsilon - d_P(c, \widetilde{c}))^2}{\frac{4}{3} \epsilon} \right) \\
& > 1 - \exp \left( - m \epsilon / 48 \right) \; .
\end{align*}
Since $|C''| \le N - 1$, by union bound, with probability at least $1 - (N - 1)
\exp(-m \epsilon/48)$, for all $c' \in C''$, $\err_T(c') > \frac{2}{3}
\epsilon$. Finally, with probability at least $1 - N \exp(-m \epsilon/48) \ge 1 -
\delta$, $\err_T(\widetilde{c}) < \frac{2}{3}\epsilon$ and for all $c' \in C''$,
$\err_T(c') > \frac{2}{3} \epsilon$.
\end{proof}

\section{Size of $\epsilon$-cover}
\label{section:epsilon-cover}

In this section, we prove $(e/\epsilon)^d$ upper bound on the size of the
$\epsilon$-cover of any concept class of Vapnik-Chervonenkis dimension $d$. To
prove our result, we need Sauer's lemma. Its proof can be found, for example,
in~\cite[Chapter 3]{Anthony-Bartlett-1999}.

\begin{lemma}[Sauer's lemma]
Let $\X$ be a non-empty domain and let $C \subseteq \{0,1\}^\X$ be a concept class
with Vapnik-Chervonenkis dimension $d$. Then, for any $S \subseteq \X$,
$$
\left| \left\{ \pi(c, S) ~:~ c \in C \right\} \right| \le \sum_{i=0}^d \binom{|S|}{i} \; .
$$
\end{lemma}

We remark that if $n \ge d \ge 1$ then
\begin{equation}
\label{equation:sauer-lemma-estimate}
\sum_{i=0}^d \binom{n}{i} \le \left( \frac{ne}{d} \right)^d
\end{equation}
where $e = 2.71828 \dots$ is the base of the natural logarithm. This follows
from the following calculation
$$
\left( \frac{d}{n} \right)^d \cdot \sum_{i=0}^d \binom{n}{i}
\le \sum_{i=0}^d \binom{n}{i} \left( \frac{d}{n} \right)^i
\le \sum_{i=0}^n \binom{n}{i} \left( \frac{d}{n} \right)^i
= \left(1 + \frac{d}{n} \right)^n \le e^d
$$
where we used in the last step that $1 + x \le e^x$ for any $x \in \R$.

\begin{theorem}[Size of $\epsilon$-cover]
Let $\X$ be a non-empty domain and let $C \subseteq \{0,1\}^\X$ be a concept
class with Vapnik-Chervonenkis dimension $d$. Let $P$ be any distribution over
$\X$. For any $\epsilon \in (0,1]$, there exists a set $C' \subseteq C$ such that
\begin{equation}
\label{equation:theorem-epsilon-cover}
|C'| \le \left( \frac{e}{\epsilon} \right)^d
\end{equation}
and for any $c \in C$ there exists $c' \in C'$ such that $d_P(c,c') \le \epsilon$.
\end{theorem}

\begin{proof}
We say that a set $B \subseteq C$ is an \emph{$\epsilon$-packing} if
$$
\forall c,c' \in B \qquad \qquad c \neq c' \quad \Longrightarrow \quad d_P(c,c') > \epsilon
$$
We claim that there exists a maximal $\epsilon$-packing. In order to show that a
maximal set exists we to appeal to Zorn's lemma. Consider the collection of all
$\epsilon$-packings. We impose partial order on them by set inclusion. Notice
that any totally ordered collection $\{ B_i ~:~ i \in I \}$ of
$\epsilon$-packings has an upper bound $\bigcup_{i \in I} B_i$ that is an
$\epsilon$-packing. Indeed, if $c,c' \in \bigcup_{i \in I} B_i$ such that $c
\neq c'$ then there exists $i \in I$ such that $c,c' \in B_i$ since $\{ B_i ~:~
i \in I \}$ is totally ordered. Since $B_i$ is an $\epsilon$-packing, $d_P(c,c') >
\epsilon$. We conclude that $\bigcup_{i \in I} B_i$ is an $\epsilon$-packing. By
Zorn's lemma, there exists a maximal $\epsilon$-packing.

Let $C'$ be a maximal $\epsilon$-packing. We claim that $C'$ is also an
$\epsilon$-cover of $C$. Indeed, for any $c \in C$ there exists $c' \in C'$ such
that $d_P(c,c') \le \epsilon$ since otherwise $C' \cup \{c\}$ would an
$\epsilon$-packing, which would contradict maximality of $C'$.

It remains to upper bound $|C'|$. Consider any finite subset $C'' \subseteq C'$.
It suffices to show an upper bound on $|C''|$ and since $C''$ is arbitrary, the
same upper bound holds for $|C'|$. Let $M = |C''|$ and let $c_1, c_2, \dots,
c_M$ be concepts in $C''$. For any $i,j \in \{1,2,\dots,M\}$, $i < j$, let
$$
A_{i,j} = \{ x \in \X ~:~ c_i(x) \neq c_j(x) \} \; .
$$
Let $X_1, X_2, \dots, X_K$ be an i.i.d. sample from $P$. We will chose $K$ later.
Since $d_P(c_i, c_j) > \epsilon$,
$$
\Pr[X_k \in A_{i,j}] > \epsilon \qquad \qquad \text{for $k=1,2,\dots,K$}.
$$
Since there are $\binom{M}{2}$ subsets $A_{i,j}$, we have
\begin{align*}
\Pr\left[\forall i,j, i < j, \ \exists k, \ X_k \in A_{i,j} \right]
& = 1 - \Pr\left[\exists i,j, i < j, \ \forall k, \ X_k \not \in A_{i,j} \right] \\
& \ge 1 - \sum_{1 \le i < j \le M} \Pr\left[\forall k, \ X_k \not \in A_{i,j} \right] \\
& \ge 1 - \sum_{1 \le i < j \le M} (1 - \epsilon)^K \\
& = 1 - \binom{M}{2} (1 - \epsilon)^K \; .
\end{align*}
For $K = \left\lceil \frac{\ln \binom{M}{2}}{\epsilon} \right\rceil +
1$, the above probability is strictly positive. This means there exists a set $S =
\{x_1, x_2, \dots, x_K\} \subseteq X$ such that $A_{i,j} \cap S$ is non-empty
for every $i < j$. This means that for every for every $i \neq j$, $c_i(S) \neq
c_j(S)$ and hence $M = |C''| = \left| \left\{ \pi(c, S) ~:~ c \in C \right\} \right|$.
Thus by Sauer's lemma
$$
M \le \sum_{i=0}^d \binom{K}{i} \; .
$$
We now show that this inequality implies that $M \le (e/\epsilon)^d$. We consider five cases.

Case 1: $d = -\infty$. Then, $M \le 0$ and inequality trivially follows.

Case 2: $d = 0$. Then, $M \le 1$ and the inequality trivially follows.

Case 3: $d \ge 1$ and $M \le 2$. Clearly, $M \le 2 \le e \le e^d \le (e/\epsilon)^d$.

Case 4: $d \ge 1$ and $M \ge 3$ and $K < d/\epsilon$. Then
$\left\lceil \frac{\ln \binom{M}{2}}{\epsilon} \right\rceil + 1 < d/\epsilon$
which implies that $\ln \binom{M}{2} < d$ and hence $\binom{M}{2} \le e^d$.
Since $M \le \binom{M}{2}$ for $M \ge 3$,
$$
M \le \binom{M}{2} \le e^d \le \left( \frac{e}{\epsilon} \right)^d \; .
$$

Case 5: $d \ge 1$ and $M \ge 3$ and $K \ge d/\epsilon$. Clearly, $K \ge d$ hence by \eqref{equation:sauer-lemma-estimate},
\begin{align*}
M
\le \sum_{i=0}^d \binom{K}{i}
\le \left( \frac{Ke}{d} \right)^d
\le \left( \frac{e}{\epsilon} \right)^d \; .
\end{align*}
\end{proof}
