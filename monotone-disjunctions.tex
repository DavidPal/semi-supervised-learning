\section{Monotone Disjunctions}
\label{section:monotone-dijsunctions}

In this section, we denote by $C_n$ the class of monotone disjunctions over the
Boolean hypercube $\X = \{0,1\}^n$. There are $2^n$ functions in $C_n$. For each
subset $I \subseteq \{1,2,\dots,n\}$ there is monotone disjunction $c_I:\X \to
\{0,1\}$ defined for any $x = (x[1],
x[2], \dots, x[n]) \in \X$ as
$$
c_I(x) = \bigvee_{i \in I} x[i] \; .
$$
If $I = \emptyset$, we define $c_I$ as the constant zero function.

For any even $n \ge 4$ and any $\epsilon \in (0,\frac{1}{2})$, we consider a
family of probability distributions $\P_{n,\epsilon}$ over the Boolean hypercube $\X =
\{0,1\}^n$. The family consists of $\binom{n}{n/2}$ probability distributions.
For each subset $J \subseteq \{1,2,\dots,n\}$ of size $|J| = n/2$
there is a probability distribution $P_J \in \P_{n,\epsilon}$. To describe a distribution $P_J$
for some $J \subseteq \{1,2,\dots,n\}$, consider a random vector $X = (X[1],
X[2], \dots, X[n])$ drawn from $P_J$. The distribution $P_J$ is a product
distribution, i.e., $\Pr[X=x] = \prod_{j=1}^n \Pr[X[j] = x[j]]$ for any $x \in
\{0,1\}^n$. The marginal distributions are
\begin{align*}
\Pr[X[j] = 1] =
\begin{cases}
1 - \sqrt[n/2 - 1]{1 - \epsilon}  & \text{if $j \in J$,} \\
1 - \epsilon & \text{if $j \not \in J$,}
\end{cases}
&& \text{for $j=1,2,\dots,n$.}
\end{align*}
Note that $1 - \sqrt[n/2-1]{1 - \epsilon} < \frac{1}{2}$ and $1 - \epsilon > \frac{1}{2}$.

\begin{proposition}[Vapnik-Chervonenkis dimension]
Vapnik-Chervonenkis dimension of the class of monotone disjunctions is $C_n$ is $n$.
\end{proposition}

\begin{proof}
Let us denote the Vapnik-Chervonenkis dimension by $d$. Recall that $d$ is the
size of the largest shattered set. Let $S$ be any shattered set of size $d$.
Then, there must be at least $2^d$ distinct functions in $C_n$. Hence, $d \le
\log_2 |C_n| = n$.

On the other hand, we construct a shattered set of size $n$. For any $i \in
\{1,2,\dots,n\}$, let $e_i = (0, \dots, 0, 1, 0, \dots, 0)$ be vector in
$\{0,1\}^n$ with all coordinate equal to zero except for the $i$-th coordinate
which is equal to $1$. We claim that the set $S = \{e_1, e_2, \dots, e_n\}$ is
shattered. For any binary vector $S' \subseteq S$, consider the set $I = \{ i
\in \{1,2,\dots,n\} ~:~ e_i \in S' \}$ and the monotone disjunction $c_I$. It is
not hard to see that $S' = \pi(c, S)$ since $c_I(e_i) = \indicator{i \in I} =
\indicator{e_i \in S'}$. Thus the set $\{e_1, e_2, \dots, e_n\}$ is shattered.
\end{proof}

\begin{lemma}
For any even $n \ge 4$, any $\epsilon \in (0,\frac{1}{2})$, and any distribution
from $\P_{n,\epsilon}$ there exists an $\epsilon$-cover of the class of monote
disjunctions $C_n$ of size $3$.
\end{lemma}

\begin{proof}
Let $J \subseteq \{1,2,\dots,n\}$ such that $|J| = n/2$ and let $P_J
\subseteq P_{n,\epsilon}$. Let $C' = \{c_J, c_\emptyset, c_{\{1,2,\dots,n\}}\}$.
We claim that $C'$ is an $\epsilon$-cover $C_n$ under the distribution $P_J$.
Let $I \subseteq \{1,2,\dots,n\}$ be arbitrary and let $c_I \in C_n$ be the
corresponding disjunction. We need to show that $d_{P_J}(c_I, c_J) \le \epsilon$
or $d_{P_J}(c_I, c_\emptyset) \le \epsilon$ or $d_{P_J}(c_I,
c_{\{1,2,\dots,n\}}) \le \epsilon$. We consider three cases.

Case 1: $I = J$. Clearly, $d_{P_J}(c_I, c_J) = 0$.

Case 2: $I \subseteq J$ and $I \neq J$. We show that $d_{P_J}(c_I, c_\emptyset) \le \epsilon$.
Note that $|I| \le n/2 - 1$. Let $X \sim P_J$. Then,
\begin{align*}
d_{P_J}(c_I, c_\emptyset)
& = \Pr[c_I(X) \neq c_\emptyset(X)] \\
& = \Pr[c_I(X) = 1] \\
& = \Pr \left[ \bigvee_{i \in I} (X[i] = 1) \right] \\
& = 1 - \Pr \left[ \bigwedge_{i \in I} (X[i] = 0) \right] \\
& = 1 - \prod_{i \in I} \Pr \left[ X[i] = 0 \right] \\
& = 1 - \prod_{i \in I} \sqrt[n/2 - 1]{1 - \epsilon} \\
& \le 1 - \bigg( \sqrt[n/2-1]{1 - \epsilon} \bigg)^{(n-1)/2} \\
& = 1 - \sqrt{1 - \epsilon} \\
& = \epsilon
\end{align*}
where the last inequality is valid for any $\epsilon \le 1/2$.

Case 3: If $I \not \subseteq J$. We show that $d_{P_J}(c_I, c_{\{1,2,\dots,n\}}) \le \epsilon$.
Since $I \not \subseteq J$, there exists $k \in I \setminus J$. Let $X \sim P_J$. Then,
\begin{align*}
d_{P_J}(c_I, c_{\{1,2,\dots,n\}})
& = \Pr [c_I(X) \neq c_{\{1,2,\dots,n\}}(x)] \\
& = \Pr [c_I(X) = 0 \ \text{and} \ X \neq (0,0,\dots,0) ] \\
& <  \Pr [c_I(X) = 0 ] \\
& = \Pr \left[ \bigwedge_{i \in I} (X[i] = 0) \right] \\
& = \prod_{i \in I} \Pr \left[ X[i] = 0 \right] \\
& \le \Pr \left[ X[k] = 0 \right] \\
& = \epsilon \; .
\end{align*}
\end{proof}

\begin{theorem}[Learning with knowledge of distribution]
For any even $n \ge 4$, there exists a distribution-dependent algorithm
such that for any any $\epsilon \in (0,\frac{1}{2})$, and any distribution from $\P_{n,\epsilon}$,
any $\delta \in (0,1)$, any target $c \in C_n$, if the algorithm gets
$$
m \ge ???
$$
labeled examples, with probability at least $1-\delta$ the algorithm $\epsilon$-learns the target.
\end{theorem}

\begin{proof}
???
\end{proof}


\subsection{Old Junk}

Consider a distribution $P_J \in \P_{n,p,p'}$. We compute the distance $d_P(c_I, c_{I'})$ between
any two monotone disjunctions $c_I$ and $c_{I'}$. For any subset $K \subseteq \{1,2,\dots,n\}$, let
$A_K$ be the event that all for all $i \in K$, $x_i = 0$.
\begin{align*}
d_P(c_I, c_J)
& = \Pr_{x \sim P}[c_I(x) \neq c_J(x)] \\
& = \Pr_{x \sim P}[A_{I \cap J} \wedge ((A_{I \setminus J} \wedge \overline{A_{J \setminus I}}) \vee (\overline{A_{I \setminus J}} \wedge A_{J \setminus I} )) ] \\
& = \Pr[A_{I \cap J}] \cdot \left\{ \Pr[A_{I \setminus J}] (1 - \Pr[A_{J \setminus I}]) + (1 - \Pr[A_{I \setminus J}]) \Pr[A_{J \setminus I}] \right\} \\
& = \Pr[A_{I \cap J}] \cdot \left\{ \Pr[A_{I \setminus J}] + \Pr[A_{J \setminus I}] - 2 \Pr[A_{I \setminus J}] \Pr[A_{J \setminus I}] \right\} \\
& = \Pr[A_I] + \Pr[A_J] - 2 \Pr[A_{I \cup J}] \\
& = \prod_{i \in I} (1 - p_i) + \prod_{j \in J} (1 - p_j) - 2 \prod_{k \in I \cup J} (1 - p_k) \; . \\
\end{align*}
