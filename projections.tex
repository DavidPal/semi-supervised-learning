\section{Projections}
\label{section:projections}

In this section, we denote by $C_n$ the class of \emph{projections} over the
domain $\X = \{0,1\}^n$. The class $C_n$ consists of $n$ functions $c_1, c_2,
\dots, c_n$ from $\{0,1\}^n$ to $\{0,1\}$. For any $i \in \{1,2,\dots,n\}$, for
any $x \in \{0,1\}^n$, the function $c_i$ is defined as $c_i((x[1], x[2], \dots,
x[n])) = x[i]$.

For any $p \in (0,\frac{1}{2})$ and $n \ge 2$, we consider a family $\P_{n,p}$
consisting of $n$ probability distributions $P_1, P_2, \dots, P_n$ over the
Boolean hypercube $\{0,1\}^n$. In order to describe the distribution $P_i$, for
some $i$, consider a random vector $X = (X[1], X[2], \dots, X[n])$ drawn from
$P_i$. The distribution $P_i$ is a product distribution, i.e., $\Pr[X = x] =
\prod_{j=1}^n \Pr[X[j] = x[j]]$ for any $x \in \{0,1\}^n$. The marginal
distributions of the coordinates are
$$
\Pr[X[j] = 1] =
\begin{cases}
\frac{1}{2} & \text{if $j = i$,} \\
p & \text{if $j\neq i$.} \\
\end{cases}
\qquad \qquad \text{for $j=1,2,\dots,n$.}
$$
Throughout the paper, we treat $p$ as a constant that does not depend on $n$.
Theorems below we will place some further constraints on $p$. As we will see any
$p \in (0,\frac{1}{16})$ works.

\begin{proposition}
\label{proposition:vc-dimension-projections}
Vapnik-Chervonenkis dimension of $C_n$ is $\lfloor \log_2 n \rfloor$.
\end{proposition}

\begin{proof}
Let us denote the Vapnik-Chervonenkis dimension by $d$. Recall that $d$ is the
size of the largest shattered set. Let $S$ be any shattered set of size $d$.
Then, there must be at least $2^d$ distinct functions in $C_n$. Hence, $d \le
\log_2 |C_n| = \log_2 n$. Since $d$ is an integer, we conclude that $d \le
\lfloor \log_2 n \rfloor$.

On the other hand, we construct a shattered set of size $\lfloor \log_2 n
\rfloor$. The set will consists of points $x_1, x_2, \dots, x_{\lfloor \log_2 n
\rfloor} \in \{0,1\}^n$. For any $i \in \{1,2,\dots,\lfloor \log_2 n \rfloor\}$
and any $j \in \{0,1,2,\dots,n-1\}$, we define $x_i[j]$ to be the $i$-th bit
in the binary representation of the number $j$. (The bit at position $i=1$ is the
least significant bit.) It is not hard to see that for any $v \in
\{0,1\}^{\lfloor \log_2 n \rfloor}$, there exists $c \in C_n$ such that $v =
(c(x_1), c(x_2), \dots, c(x_{\lfloor \log_2 n \rfloor}))$. Indeed, given $v$,
let $k \in \{0,1,\dots,2^{\lfloor \log_2 n \rfloor} - 1\}$ be the number with
binary representation $v$, then we can take $c = c_{k+1}$.
\end{proof}

The following theorem states that under any distribution from $\P_{n,p}$, if the
distribution is known, for learning any target from $C_n$, the number of
labeled examples that are required does not depend on $n$. In other
words, the distribution-dependent complexity is $O(1)$.

\begin{theorem}[Learning with knowledge of the distribution]
For any $n \ge 2$, there exists a distribution-dependent algorithm such that
for any $p \in (0,\frac{1}{4})$, any distribution from $\P_{n,p}$, any $\delta
\in (0,1)$, any target function $c \in C_n$,
if the algorithm gets
$$
m \ge \frac{2\ln(1/\delta)}{(1 - 4p(1-p))^2}
$$
labeled examples, with probability at least $1 - \delta$, it $2p$-learns the target.
\end{theorem}

\begin{proof}
Given a distribution $P_i \in \P_{n,p}$ for some $i \in \{1,2,\dots,n\}$, the
algorithm chooses two candidate functions $c_i, c_j \in C_n$ where $j$ is
arbitrary index in $\{1,2,\dots,n\} \setminus \{j\}$. Given a sample $T =
((x_1, y_1), (x_2, y_2), \dots, (x_m,y_m))$ it outputs either $c_i$ or $c_j$
depending on which one has smaller empirical error. Recall that
empirical error of a classifier $h:\X \to \{0,1\}$ is
$$
\widehat \err(h) = \frac{1}{m} \sum_{t=1}^m \indicator{y_t \neq h(x_t)} \; .
$$
If $x_1, x_2, \dots, x_m$ are drawn i.i.d. from $P_i$
and the target function is $c_k \in C_n$ then
$\Exp[\indicator{y_t \neq h(x_t)}] = \Exp[\widehat \err(h)] = d_{P_i}(c_k, h)$.
There are three cases. Either $k = i$, $k = j$, or $k \not \in \{i,j\}$.

\emph{Case $k=i$.} With probability one, $\widehat \err(c_i) = 0$.
Since $\indicator{y_t \neq c_j(x_t)}$ is a Bernoulli variable with parameter
$d_{P_i}(c_k, c_j) = d_{P_i}(c_i, c_j) = \frac{1}{2}$,
using Hoeffding bound with $\epsilon = \frac{1}{2}$,
$$
\Pr[\widehat \err(c_j) > \widehat \err(c_i)] = \Pr[\widehat \err(c_j) > 0] = \Pr[\widehat \err(c_j) > 1/2 - \epsilon] \ge 1 - e^{-m/2} \ge 1 - \delta \; .
$$
Therefore, with probability $1 - \delta$, the algorithm outputs the correct target $c_i$.

\emph{Case $k=j$.} With probability one, $\widehat \err(c_j) = 0$.
Since $\indicator{y_t \neq c_i(x_t)}$ is a Bernoulli variable with parameter
$d_{P_i}(c_k, c_i) = d_{P_i}(c_j, c_i) = \frac{1}{2}$,
using Hoeffding bound with $\epsilon = \frac{1}{2}$,
$$
\Pr[\widehat \err(c_i) > \widehat \err(c_j)] = \Pr[\widehat \err(c_i) > 0] = \Pr[\widehat \err(c_i) > 1/2 - \epsilon] \ge 1 - e^{-m/2} \ge 1 - \delta \; .
$$
Therefore, with probability $1 - \delta$, the algorithm outputs the correct target $c_j$.

\emph{Case $k \not \in \{i,j\}$.}
Since $\indicator{y_t \neq c_i(x_t)}$ is a Bernoulli variable with parameter
$d_{P_i}(c_k, c_i) = \frac{1}{2}$ and $\indicator{y_t \neq c_j(x_t)}$ is a Bernoulli variable with parameter
$d_{P_i}(c_k, c_j) = 2p(1-p)$, and hence $\indicator{y_t \neq c_i(x_t)} - \indicator{y_t \neq c_j(x_t)}$
is a random variable that lies in $[-1,1]$ and has mean $\frac{1}{2} - 2p(1-p)$.
Using Hoeffding bound with $\epsilon = \frac{1}{2} - 2p(1-p)$,
$$
\Pr[\widehat \err(c_i) > \widehat \err(c_j)] = \Pr[\widehat \err(c_i) - \widehat \err(c_j) > 0] \ge 1 - e^{-m(1 - 4p(1-p))^2/2} \ge 1 - \delta \; .
$$
Therefore, with probability $1 - \delta$, the algorithm outputs $c_j$ which satisfies
$$
d_{P_i}(c_j, c_k) = 2p(1-p) < 2p \; .
$$
\end{proof}

The next theorem states that without knowing the distribution,
learning a target under a distribution from $\P_{n,p}$
requires at least $\Omega(\log n)$ labeled examples.

\begin{theorem}[Learning without knowledge of the distribution]
For any distribution-independent algorithm, any $p \in (0,\frac{1}{16})$, and any
$n \ge 600/p^3$ there exists a distribution $P \in \P_{n,p}$ and a target
concept $c \in C_n$ such that if the number of labeled examples is less than or equal to
$$
\frac{\ln n}{3 \ln (1/p)}
$$
the algorithm fails to $2p$-learn the
target concept with probability more than $\frac{1}{8} - 2p$.
\end{theorem}

\begin{proof}
Let $A$ be any learning algorithm. For ease of notation, we formalize it is as a function
$$
A:\bigcup_{m=0}^\infty \left(\{0,1\}^{m \times n} \times \{0,1\}^m\right) \to \{0,1\}^{\{0,1\}^n} \; .
$$
The algorithm receives an $m \times n$ matrix and a binary vector of length $m$.
The rows of the matrix corresponds to unlabeled examples and the vector encodes
the labels. The output of the $A$ is any function from $\{0,1\}^n \to \{0,1\}$.

Fix the number of examples $m$ to be any non-negative integer less than or equal
to  $\frac{\ln n}{3 \ln(1/p)}$. We demonstrate the existence of a pair $(P,c)
\in \P_{n,p} \times C_n$ which cannot be learned with $m$ samples by
probabilistic method.

Let $I$ be chosen uniformly at random from $\{1,2,\dots,n\}$. We consider distribution
$P_I \in \P_{n,p}$ and target $c_I \in C_n$. Let $X_1, X_2, \dots, X_m$ be an
i.i.d. sample from $P_I$ and let $Y_1 = c_I(X_1), Y_2 = c_I(X_2), \dots, Y_m =
c_I(X_m)$ be the target labels. Let $X$ be the $m \times n$ matrix with entries
$X_i[j]$ and let $Y = (Y_1, Y_2, \dots, Y_m)$ be the vector of labels. The
output of the algorithm is $A(X,Y)$. We will show that
\begin{equation}
\label{equation:failure-probability}
\Exp \left[d_{P_I}(c_I, A(X,Y)) \right] \ge \frac{1}{8} \; .
\end{equation}
This means that there exists $i \in \{1,2,\dots,n\}$ such that
$$
\Exp \left[d_{P_i}(c_i, A(X,Y)) \right] \ge \frac{1}{8}
$$
which, by Proposition~\ref{proposition:error-probability-vs-expected-error} with $\epsilon=2p$, implies that
$$
\Pr \left[ d_{P_i}(c_i, A(X,Y)) > 2p \right] \ge \frac{\frac{1}{8} - 2p}{1 - 2p} > \frac{1}{8} - 2p \; .
$$

It remains to prove \eqref{equation:failure-probability}.
Let $Z$ be a test sample drawn from $P_I$. That is, conditioned on $I$, the sequence $X_1, X_2, \dots, X_m, Z$ is i.i.d. drawn from $P_I$.
Then, by Proposition~\ref{proposition:bayes},
\begin{align}
& \Exp \left[d_{P_I}(c_I, A(X,Y))\right] \notag \\
& = \Pr\left[ A(X,Y)(Z) \neq c_I(Z) \right] \notag \\
& \ge \sum_{\substack{x \in \{0,1\}^{m \times n} \\ y \in \{0,1\}^m \\ z \in \{0,1\}^n}} \left( \frac{1}{2} - \left| \frac{1}{2} - \Exp\left[ c_I(Z) \, \middle| \, X = x, Y = y, Z = z \right] \right| \right) \cdot \Pr \left[X = x, Y = y, Z = z \right] \label{equation:expected-error-lower-bound} \; .
\end{align}
We need to compute $\Exp\left[ c_I(Z) \, \middle| \, X = x, Y = y, Z = z \right]$.
For that we need some additional notation.
For any matrix $x \in \{0,1\}^{m \times n}$, let $x[1], x[2], \dots, x[n]$ be its columns.
For any matrix $x \in \{0,1\}^{m \times n}$ and vector $y \in \{0,1\}^m$ let
$$
k(x,y) = \{ i \in \{1,2,\dots,n\} ~:~ x[i] = y \}
$$
be the set of indices of columns of $x$ equal to the vector $y$. The key
observation is that due to the symmetry of the uniform distribution from which
$I$ chosen, for any $i = 1,2,\dots,n$,
\begin{align*}
\Pr \left[I = i \, \middle| \, X = x, Y = y \right]
=
\begin{cases}
\frac{1}{|k(x,y)|} & \text{if $i \in k(x,y)$,} \\
0 & \text{if $i \not \in k(x,y)$,} \\
\end{cases}
\end{align*}
Conditioned on $I$, the variables $Z$ and $(X,Y)$ are independent. Thus,
for any $x \in \{0,1\}^n$, and $i = 1,2,\dots,n$,
\begin{align*}
\Pr \left[Z = z \, \middle| \, I = i, X = x, Y = y \right]
& = \Pr \left[Z = z \, \middle| \, I = i \right] \\
& =
\begin{cases}
\frac{1}{2} p^{\norm{z}_1 - 1} (1 - p)^{n - \norm{z}_1} & \text{if $z[i] = 1$,} \\
\frac{1}{2} p^{\norm{z}_1} (1-p)^{n-1 - \norm{z}_1}& \text{if $z[i] = 0$.} \\
\end{cases}
\end{align*}
This allows us to compute the conditional probability
\begin{align*}
& \Pr \left[I = i, Z = z \, \middle| \, X = x, Y = y \right] \\
& \quad = \Pr \left[Z = z \, \middle| \, I = i, X = x, Y = y \right] \cdot \Pr \left[I = i \, \middle| \, X = x, Y = y \right] \\
& \quad =
\begin{cases}
\frac{1}{2|k(x,y)|} p^{\norm{z}_1 - 1} (1 - p)^{n - \norm{z}_1} & \text{if $i \in k(x,y)$ and $z[i] = 1$,} \\
\frac{1}{2|k(x,y)|} p^{\norm{z}_1} (1 - p)^{n - 1 - \norm{z}_1} & \text{if $i \in k(x,y)$ and $z[i] = 0$,} \\
0 & \text{if $i \not \in k(x,y)$.} \\
\end{cases}
\end{align*}
Let
$$
s(x,y,z) = \{ i \in k(x,y) ~:~ z[i] = 1 \} \; .
$$
and note that $s(x,y,z) \subseteq k(x,y)$.
Then,
\begin{align*}
& \Pr \left[Z = z \, \middle| \, X = x, Y = y \right] \\
& \quad = \sum_{i=1}^n \Pr \left[Z = z, I = i \, \middle| \, X = x, Y = y \right] \\
& \quad = \sum_{i \in k(x,y)} \Pr \left[Z = z, I = i \, \middle| \, X = x, Y = y \right] \\
& \quad = \sum_{i \in s(x,y,x)} \Pr \left[Z = z, I = i \, \middle| \, X = x, Y = y \right] + \sum_{i \in k(x,y) \setminus s(x,y,z)} \Pr \left[Z = z, I = i \, \middle| \, X = x, Y = y \right] \\
& \quad = \frac{1}{2|k(x,y)|} \cdot |s(x,y,z)| \cdot p^{\norm{z}_1 - 1} (1 - p)^{n - \norm{z}_1} + \frac{1}{2|k(x,y)|} \cdot (|k(x,y)| - |s(x,y,z)|) \cdot p^{\norm{z}_1} (1 - p)^{n - 1 - \norm{z}_1} \\
& \quad = \frac{p^{\norm{z}_1 - 1} (1 - p)^{n - 1 - \norm{z}_1}}{2|k(x,y)|} \cdot \left( |s(x,y,z)| \cdot (1 - 2p) + |k(x,y)| \cdot p \right) \; .
\end{align*}
Hence,
\begin{align*}
& \Exp\left[ c_I(Z) \, \middle| \, X = x, Y = y, Z = z \right] \\
& \quad = \Pr \left[ Z[I] = 1 \, \middle| \, X = x, Y = y, Z = z \right] \\
& \quad = \frac{\displaystyle \Pr \left[ Z[I] = 1, Z = z \, \middle| \, X = x, Y = y \right]}{\displaystyle \Pr \left[ Z = z \, \middle| \, X = x, Y = y \right]} \\
& \quad = \frac{\displaystyle \sum_{i=1}^n \Pr \left[ I = i, Z[i] = 1, Z = z \, \middle| \, X = x, Y = y \right]}{\displaystyle \Pr \left[ Z = z \, \middle| \, X = x, Y = y \right]} \\
& \quad = \frac{\displaystyle \frac{|s(x,y,z)|}{2|k(x,y)|} \cdot p^{\norm{z}_1 - 1} (1 - p)^{n - \norm{z}_1}}{\displaystyle \frac{p^{\norm{z}_1 - 1} (1 - p)^{n - 1 - \norm{z}_1}}{2|k(x,y)|} \cdot \left( |s(x,y,z)| \cdot (1 - 2p) + |k(x,y,z)| \cdot p \right) } \\
& \quad = \frac{\displaystyle |s(x,y,z)| \cdot (1 - p)}{\displaystyle |s(x,y,z)| \cdot (1 - 2p) + |k(x,y)| \cdot p } \\
& \quad = \frac{\displaystyle 1 - p}{\displaystyle 1 - 2p + \frac{|k(x,y)| \cdot p}{|s(x,y,z)|}} \\
\end{align*}

We now show that the last expression is close to $1/2$. More precisely, if
\begin{equation}
\frac{|k(x,y)| \cdot p}{|s(x,y,z)|} \in \left[\frac{5}{6}, 2 \right]
\end{equation}
then since $p \in (0,\frac{1}{4})$,
$$
\frac{\displaystyle 1 - p}{\displaystyle 1 - 2p + \frac{|k(x,y)| \cdot p}{|s(x,y,z)|}} \ge
\frac{\displaystyle 1 - p}{\displaystyle 1 - 2p + 2} \ge \frac{\displaystyle 1 - 1/4}{\displaystyle 1 + 2} = \frac{1}{4}
$$
and
$$
\frac{\displaystyle 1 - p}{\displaystyle 1 - 2p + \frac{|k(x,y)| \cdot p}{|s(x,y,z)|}} \le
\frac{\displaystyle 1 - p}{\displaystyle 1 - 2p + 5/6} \le \frac{\displaystyle 1}{\displaystyle 1 - 1/2 + 5/6} = \frac{3}{4} \; .
$$
We now substitute this into the \eqref{equation:expected-error-lower-bound}. We have
\begin{align*}
& \sum_{\substack{x \in \{0,1\}^{m \times n} \\ y \in \{0,1\}^m \\ z \in \{0,1\}^n}} \frac{1}{2} - \left| \frac{1}{2} - \Exp\left[ c_I(Z) \, \middle| \, X = x, Y = y, Z = z \right] \right| \cdot \Pr \left[X = x, Y = y, Z = z \right] \\
& = \sum_{\substack{x \in \{0,1\}^{m \times n} \\ y \in \{0,1\}^m \\ z \in \{0,1\}^n}} \frac{1}{2} - \left| \frac{1}{2} - \frac{\displaystyle 1 - p}{\displaystyle 1 - 2p + \frac{|k(x,y)| \cdot p}{|s(x,y,z)|}} \right| \cdot \Pr \left[X = x, Y = y, Z = z \right] \\
& \ge
\sum_{\substack{x \in \{0,1\}^{m \times n} \\ y \in \{0,1\}^m \\ z \in \{0,1\}^n \\ \frac{|k(x,y,z)| p}{|s(x,y,z)|} \in [\frac{5}{6},2]}} \left( \frac{1}{2} - \left| \frac{1}{2} - \frac{\displaystyle 1 - p}{\displaystyle 1 - 2p + \frac{|k(x,y)| \cdot p}{|s(x,y,z)|}} \right|  \right) \cdot \Pr \left[X = x, Y = y, Z = z \right] \\
& \ge
\sum_{\substack{x \in \{0,1\}^{m \times n} \\ y \in \{0,1\}^m \\ z \in \{0,1\}^n \\ \frac{|k(x,y,z)| p}{|s(x,y,z)|} \in [\frac{5}{6},2]}} \left( \frac{1}{2} - \frac{1}{4} \right) \cdot \Pr \left[X = x, Y = y, Z = z \right] \\
& =
\frac{1}{4} \Pr \left[ \frac{|k(X,Y)| \cdot p}{|s(X,Y,Z)|} \in \left[\frac{5}{6}, 2 \right]  \right] \\
\end{align*}
In order to prove \eqref{equation:failure-probability}, we need to show that
$\frac{|k(X,Y)| \cdot p}{|s(X,Y,Z)|} \in \left[\frac{5}{6}, 2 \right]$ with
probability at least $1/2$. To that end, we define two additional random
variables
$$
K = |k(X,Y)| \qquad \text{and} \qquad S = |s(X,Y,Z)| \; .
$$
The condition $\frac{|k(X,Y)| \cdot p}{|s(X,Y,Z)|} \in \left[\frac{5}{6}, 2 \right]$ is equivalent to
\begin{equation}
\label{equation:ratio-condition}
\frac{1}{2} p \le \frac{S}{K} \le \frac{6}{5} p \; .
\end{equation}

First, we lower bound $K$. For any $y \in \{0,1\}^m$ and any $i,j \in \{1,2,\dots,n\}$,
$$
\Pr \left[ j \in k(X,Y)  \, \middle| \, Y = y, I = i \right]
=
\begin{cases}
1 & \text{if $j = i$,} \\
p^{\norm{y}_1} (1 - p)^{m - \norm{y}_1} & \text{if $j \neq j$.}
\end{cases}
$$
Conditioned on $Y = y$ and $I=i$, the random variable $K - 1 = |k(X,Y) \setminus \{I\}|$ is
a sum of $n-1$ Bernoulli variables with parameter $p^{\norm{y}_1} (1 - p)^{m - \norm{y}_1}$, one for each column except for column $i$.
Hoeffding bound with $\epsilon = p^m/2$ and the loose lower bound $p^{\norm{y}_1} (1 - p)^{m - \norm{y}_1} \ge p^m$ gives
\begin{align*}
\Pr \left[ \frac{K - 1}{n - 1} > \frac{p^m}{2}  \, \middle| \,  Y = y, I = i  \right]
& = \Pr \left[ \frac{K - 1}{n - 1} > p^m - \epsilon  \, \middle| \,  Y = y, I = i  \right] \\
& \ge \Pr \left[ \frac{K - 1}{n - 1} > p^{\norm{y}_1} (1 - p)^{m - \norm{y}_1} - \epsilon  \, \middle| \,  Y = y, I = i  \right] \\
& \ge 1 - e^{-2(n-1) \epsilon^2} \; .
\end{align*}
Since $m \le \frac{\ln n}{3 \ln (1/p)}$, we lower bound $\epsilon = \frac{p^m}{2}$ as
$$
\epsilon = p^m/2 > \frac{1}{2} p^\frac{\ln n}{3 \ln(1/p)} = \frac{1}{2\sqrt[3]{n}} \; .
$$
Since the lower bound is uniform for all choices of $y$ and $i$, we can remove
the conditioning and conclude that
$$
\Pr \left[ K > 1 + \frac{(n-1)}{2\sqrt[3]{n}} \right] \ge 1 - \exp \left(- \frac{(n-1)}{2 n^{2/3}} \right) \; .
$$
For $n \ge 25$, we can simplify it further to
$$
\Pr \left[ K \ge \frac{n^{2/3}}{2} \right] \ge \frac{3}{4} \; .
$$

Second, conditioned on $K=r$, the random variable $S$
is a sum of $r-1$ Bernoulli random variables with parameter $p$
and one Bernoulli random variable with parameter $1/2$. Hoeffding bound for any $\epsilon > 0$
gives that
\begin{align*}
\Pr \left[ \left| \frac{S}{K} - \frac{p(K - 1) + 1/2}{K} \right| < \epsilon \, \middle| \, K = r \right] \ge 1 - 2 e^{-2 r \epsilon^2} \; .
\end{align*}
Thus,
\begin{align*}
\Pr \left[ \left| \frac{S}{K} - \frac{p(K - 1) + 1/2}{K} \right| < \epsilon \ \text{and} \ K \ge \frac{n^{2/3}}{2} \right]
& \ge \sum_{r = \lceil n^{2/3} / 2 \rceil}^n \Pr \left[ \left| \frac{S}{K} - \frac{p(K - 1) + 1/2}{K} \right| < \epsilon  \, \middle| \,  K = r \right] \cdot \Pr[K = r] \\
& \ge \sum_{r = \lceil n^{2/3} / 2 \rceil}^n \left( 1 - 2 e^{-2 r \epsilon^2} \right) \cdot \Pr[K = r] \\
& \ge \left( 1 - 2 e^{-n^{2/3}  \epsilon^2 / 2} \right) \cdot \Pr \left[ K \ge \frac{n^{2/3}}{2} \right] \; .
\end{align*}
We choose $\epsilon = p/4$. Since $n \ge 600/p^3$, we have $e^{-n^{2/3}  \epsilon^2 / 2} < \frac{1}{8}$
\begin{align*}
\Pr \left[ \left| \frac{S}{K} - \frac{p(K - 1) + 1/2}{K} \right| < \epsilon \ \text{and} \ K \ge \frac{n^{2/3}}{2} \right]
& \ge \left( 1 - 2 e^{-n^{2/3}  \epsilon^2 / 2} \right) \cdot \Pr \left[ K \ge \frac{n^{2/3}}{2} \right] \\
& \ge \frac{3}{4} \left( 1 - 2 e^{-n^{2/3}  \epsilon^2 / 2} \right) \\
& > \frac{3}{4} \left( 1 - \frac{1}{4} \right) \\
& = \frac{9}{16}  \\
& > \frac{1}{2} \; .
\end{align*}
We claim that $\epsilon = p/4$,
$\left| \frac{S}{K} - \frac{p(K - 1) + 1/2}{K} \right| < \epsilon$
and $K \ge \frac{n^{2/3}}{2}$ imply \eqref{equation:ratio-condition}.
To see that, note that $\left| \frac{S}{K} - \frac{p(K - 1) + 1/2}{K} \right| < \epsilon$ is equivalent to
$$
\frac{p(K - 1) + 1/2}{K} - \epsilon < \frac{S}{K} < \frac{p(K - 1) + 1/2}{K} + \epsilon
$$
which implies that
$$
p \left(1 - \frac{1}{K} \right) - \epsilon < \frac{S}{K} < p \left(1 - \frac{1}{K} \right) + \frac{1}{2K} + \epsilon \; .
$$
Since $K \ge \frac{n^{2/3}}{2}$ and $n \ge 25$ we have $K > 4$, which implies that
$$
\frac{3}{4} p - \epsilon < \frac{S}{K} < \frac{3}{4} p + \frac{1}{2K} + \epsilon \; .
$$
Since $K \ge \frac{n^{2/3}}{2}$ and $n \ge \frac{12}{p^{3/2}}$ we have $K > \frac{5}{2p}$, which implies that
$$
\frac{3}{4} p - \epsilon < \frac{S}{K} < \frac{3}{4} p + \frac{p}{5} + \epsilon \; .
$$
Since $\epsilon = p/4$, condition \eqref{equation:ratio-condition} follows.
\end{proof}
