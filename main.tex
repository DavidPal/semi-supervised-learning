\documentclass[12pt]{article}

\usepackage{fullpage}
\usepackage{natbib}
\usepackage{amssymb,amsthm,amsmath}

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{theorem}[proposition]{Theorem}

\renewcommand{\P}{\mathcal{P}}
\newcommand{\indicator}[1]{\mathbf{1}[{#1}]}
\newcommand{\x}{\mathbf{x}}

\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\Exp}{\mathbf{E}}

\title{Semi-supervised learning}
\author{Alexandr Golovenv \and D\'avid P\'al}

\begin{document}

\maketitle

\section{Realizable Case: Projections}

Let $C_n$ be the class of projections over the domain $\{0,1\}^n$. The concept
class $C_n$ consists of $n+1$ functions $c_0, c_1, c_2, \dots, c_n$ from
$\{0,1\}^n$ to ${0,1}$. The function $c_0$ is the constant zero function. For $i
\ge 1$, the function $c_i$ is defined by $c_i((x_1, x_2, \dots, x_n)) = x_i$.
The Vapnik-Chervonenkis dimension of $C_n$ is $d = \lfloor \log_2(n+1) \rfloor$;
see Proposition~\ref{proposition:vc-dimension-projections} below.

\begin{proposition}
\label{proposition:vc-dimension-projections}.
Vapnik-Chervonenkis dimension of $C_n$ is $\lfloor \log_2(n+1) \rfloor$.
\end{proposition}

\begin{proof}
Vapnik-Chervonenkis dimension is the size of the largest shattered set; let us
denote it by $d$. Let $S$ be a shattered set of size $d$. Then, there must be at
least $2^d$ distinct functions in $C_n$. Hence, $d \le \log_2 |C_n| =
\log_2(n+1)$. Since $d$ is an integer, we conclude that $d \le \lfloor \log_2(n+1)
\rfloor$.

On the other hand, we construct a shattered set of size $\lfloor \log_2(n+1)
\rfloor$. The set will consists of points $x_1, x_2, \dots, x_{\lfloor
\log_2(n+1) \rfloor} \in \{0,1\}^n$. We define $x_{i,j}$ to be the $j$-th bit of
the number $i$ in its binary representation. (The first bit is the least
significant bit, and $\lfloor \log_2(n+1) \rfloor$-th bit is the most significant
bit.) It is not hard to see that for any $v \in \{0,1\}^{\lfloor \log_2(n+1)
\rfloor}$, there exists $c \in C_n$ such that $v = (c(x_1), c(x_2), \dots,
c(x_d))$. Indeed, let $i \in \{0,1,\dots,n\}$ be the number with binary
representation $v$, then we have $v = (c_i(x_1), c_i(x_2), \dots, c_i(x_d))$.
\end{proof}

\cite{Hanneke-2016} showed that for any class of $C$ of Vapnik-Chervonenkis
dimension $d$ there exists an algorithm that $\epsilon$-learns any from
$O\left(\frac{d + \log(1/\delta)}{\epsilon}\right)$ examples under any
distribution with probability at least $1-\delta$. Futhermore, for any algorithm
and any class $C$ of Vapnik-Chervonenkis dimension $d$ there exists a
distribution over the domain and a concept which requires at least $\Omega
\left(\frac{d + \log(1/\delta)}{\epsilon}\right)$ samples to $\epsilon$-learn
with at least probability $1 - \delta$.

We construct a family of probability distributions $\P_n$ over ${0,1}^n$ such that:
\begin{itemize}
\item For any $P \in \P_n$ there exists an algorithm that learns
any target function $c \in C_n$ from $O(1/\epsilon^2 + \log(1/\delta)/\epsilon)$ examples.
\item For any algorithm, there exists
a probability distribution $P \in \P_n$ and a target function $c \in C_n$
that requires at least $\Omega(\log n)$ examples to $\epsilon$-learn the target function.
\end{itemize}



The family $\P_n$ consists of $n!$ probability distributions. Each distribution
corresponds to a permutation $\sigma:\{1,2,\dots,n\} \to \{1,2,\dots,n\}$. Given
a permutation $\sigma$, the corresponding distribution is $P_\sigma$.
A sample $x = (x_1, x_2, \dots, x_n)$ from $P_\sigma$ is drawn as follows:
$x_1, x_2, \dots, x_n$ are independent Bernoulli random variables
where
$$
\Pr[x_{\sigma(i)} = 1] = p_i = 1/\log_2(3 + i) \; .
$$

\begin{lemma}[Fixed distribution learning]
For any distribution $P \in \P_n$ there exists an algorithm that
learns any target function $c \in C_n$ from $O(1/\epsilon^2 + \log(1/\delta)/\epsilon)$
examples.
\end{lemma}

\begin{proof}
The proof relies on a result of \cite{Benedek-Itai-1991}. The result states that
$(C_n,P)$ has $\tfrac{\epsilon}{2}$-cover of size $N$, then any target function
$c \in C_n$ is learnable from $O(\log(N/\delta)/\epsilon)$ examples. We show
that $(C_n,P)$ has $\tfrac{\epsilon}{2}$-cover of size $N = O(2^{2/\epsilon})$
with respect to the pseudometric $d_P(c,c') = \Pr_{x \sim P}[c(x) \neq c'(x)]$.

Since $P \in \P_n$ we know that there exists $\sigma \in S_n$
such that $P = P_\sigma$. Let $N = \lceil 2^{2/\epsilon} - 3 \rceil$.
We show that $S = \{c_0, c_{\sigma(1)}, \dots, c_{\sigma(N-1)}\}$ is
an $\tfrac{\epsilon}{2}$-cover of $C_n$. Indeed, for any $i \ge N$,
\begin{align*}
d_P(c_0, c_{\sigma(i)})
& = \Pr_{x \sim P}[c_0(x) \neq c_{\sigma(i)}(x)] \\
& = \Pr_{x \sim P}[c_{\sigma(i)}(x) = 1] \\
& = \Pr_{x \sim P}[x_{\sigma(i)} = 1] \\
& = p_i \\
& = \frac{1}{\log_2(3+i)} \\
& \le \frac{1}{\log_2(3+N)} \\
& \le \epsilon/2 \; .
\end{align*}
Hence, for any $c \in C_n$ there exists $c' \in S$ such that $d_P(c,c') \le \epsilon/2$
and thus $S$ is an $\tfrac{\epsilon}{2}$-cover of $C_n$.
\end{proof}

For $i \neq j$,
\begin{align*}
d_{P_\sigma}(c_{\sigma(i)}, c_{\sigma(j)})
& = \Pr_{x \sim P_{\sigma}}[c_{\sigma(i)}(x) \neq c_{\sigma(j)}(x)] \\
& = \Pr_{x \sim P_{\sigma}}[x_{\sigma(i)} \neq x_{\sigma(j)}] \\
& = p_i (1 - p_j) + (1 - p_i) p_j \\
& \ge (p_i + p_j) / 2
\end{align*}

\begin{lemma}
For any algorithm, there exists a distribution $P \in \P_n$
and a target concept $c \in C_n$ that requires at least $\Omega(\log n)$
examples to learn the target.
\end{lemma}

\begin{proof}
We demostrate the existence of a pair $(P,c) \in \P_n \times C_n$ by
probabilistic method. Let $\sigma \in S_n$ be chosen independently at random
from $S_n$. We consider distribution $P = P_\sigma$ and target concept $c =
c_\sigma(1)$.

We can formalize any (possibly improper) learning algorithm as a function $f:D_n
\times \bigcup_{m=0}^\infty (D_n \times \{0,1\})^m \to \{0,1\}$.

TODO
\end{proof}

\section{Realizable Case: Monotone Disjuctions}

Consider the class $C_n$ of monotone disjuctions over $D_n = \{0,1\}^n$.
There are $2^n$ functions in $C_n$. For each subset $I \subseteq \{1,2,\dots,n\}$
there is monotone disjuction $c_I:D_n \to \{0,1\}$ defined by
$$
c_I(x) = \bigvee_{i \in I} x_i \; .
$$
We define $c_\emptyset$ to be constant zero function.

We consider the same family of distributions $\P_n$ as in the previous section.
Let $P = P_{Identity} \in \P_n$. We compute the distance $d_P(c_I, c_J)$
between any two monotone disjuctions. For any subset
$K \subseteq \{1,2,\dots,n\}$, let $A_K$ be the event that all for all $i \in K$, $x_i = 0$.
\begin{align*}
d_P(c_I, c_J)
& = \Pr_{x \sim P}[c_I(x) \neq c_J(x)] \\
& = \Pr_{x \sim P}[A_{I \cap J} \wedge ((A_{I \setminus J} \wedge \overline{A_{J \setminus I}}) \vee (\overline{A_{I \setminus J}} \wedge A_{J \setminus I} )) ] \\
& = \Pr[A_{I \cap J}] \cdot \left\{ \Pr[A_{I \setminus J}] (1 - \Pr[A_{J \setminus I}]) + (1 - \Pr[A_{I \setminus J}]) \Pr[A_{J \setminus I}] \right\} \\
& = \Pr[A_{I \cap J}] \cdot \left\{ \Pr[A_{I \setminus J}] + \Pr[A_{J \setminus I}] - 2 \Pr[A_{I \setminus J}] \Pr[A_{J \setminus I}] \right\} \\
& = \Pr[A_I] + \Pr[A_J] - 2 \Pr[A_{I \cup J}] \\
& = \prod_{i \in I} (1 - p_i) + \prod_{j \in J} (1 - p_j) - 2 \prod_{k \in I \cup J} (1 - p_k) \; . \\
\end{align*}

\section{Agnostic Model}

Let $C$ be a concept class over a domain $X$. Let $P$ be any distribution over
$X$. We define the ``disagreement'' pseudometric $d_P(c,c') = \Pr_{(x,y) \sim
P}[c(x) = c'(x)]$ on $C$. We denote by $N(C,P,\epsilon,)$ be the size of the
smallest $\epsilon$-cover of $C$ with respect to $d_P$.

Let $R(c) = \Pr_{(x,y) \sim P}[c(x) \neq y]$ be the risk.
For an i.i.d. sample $(X_1, Y_1), \dots, (X_n, Y_n)$ be an i.i.d. sample
from $P$, we define the empirical risk
$$
R_n(c) = \sum_{i=1}^n \indicator{c(X_i) \neq Y_i} \; .
$$
Finally, let $c_n = \argmin_{c \in C} R_n(c)$.

\bibliography{biblio}
\bibliographystyle{plainnat}

\end{document}
