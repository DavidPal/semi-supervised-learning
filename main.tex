\documentclass[12pt]{article}

\usepackage{fullpage}
\usepackage{natbib}
\usepackage{amssymb,amsthm,amsmath}

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}

\renewcommand{\P}{\mathcal{P}}
\newcommand{\indicator}[1]{\mathbf{1}\left[{#1}\right]}
\newcommand{\x}{\mathbf{x}}

\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\Exp}{\mathbf{E}}

\title{Semi-supervised learning}
\author{Alexander Golovnev \and D\'avid P\'al}

\begin{document}

\maketitle

\section{Realizable Case: Projections}

Let $C_n$ be the class of projections over the domain $\{0,1\}^n$. The concept
class $C_n$ consists of $n$ functions $c_1, c_2, \dots, c_n$ from $\{0,1\}^n$ to
$\{0,1\}$. For any $i \in \{1,2,\dots,n\}$, the function $c_i$ is defined by
$c_i((x_1, x_2, \dots, x_n)) = x_i$. The Vapnik-Chervonenkis dimension of $C_n$
is $d = \lfloor \log_2 n \rfloor$; see
Proposition~\ref{proposition:vc-dimension-projections} below.

\begin{proposition}
\label{proposition:vc-dimension-projections}
Vapnik-Chervonenkis dimension of $C_n$ is $\lfloor \log_2 n \rfloor$.
\end{proposition}

\begin{proof}
Vapnik-Chervonenkis dimension is the size of the largest shattered set; let us
denote it by $d$. Let $S$ be a shattered set of size $d$. Then, there must be at
least $2^d$ distinct functions in $C_n$. Hence, $d \le \log_2 |C_n| =
\log_2 n$. Since $d$ is an integer, we conclude that $d \le \lfloor \log_2 n
\rfloor$.

On the other hand, we construct a shattered set of size $\lfloor \log_2 n
\rfloor$. The set will consists of points $x_1, x_2, \dots, x_{\lfloor \log_2 n
\rfloor} \in \{0,1\}^n$. For any $i \in \{1,2,\dots,\lfloor \log_2 n \rfloor\}$
and any $j \in \{1,2,\dots,n\}$, we define $x_{i,j}$ to be the $i$-th bit of the
number $j-1$ in its binary representation. (The bit at position $i=1$ is the
least significant bit.) It is not hard to see that for any $v \in
\{0,1\}^{\lfloor \log_2 n \rfloor}$, there exists $c \in C_n$ such that $v =
(c(x_1), c(x_2), \dots, c(x_{\lfloor \log_2 n \rfloor}))$. Indeed, given $v$,
let $k \in \{0,1,\dots,2^{\lfloor \log_2 n \rfloor} - 1\}$ be the number with
binary representation $v$, then we can take $c = c_{k+1}$.
\end{proof}

\cite{Hanneke-2016} showed that for any class $C$ of Vapnik-Chervonenkis
dimension $d$ there exists an algorithm that $\epsilon$-learns any target function
under any domain distribution  from $O\left(\frac{d + \log(1/\delta)}{\epsilon}\right)$
examples with probability at least $1-\delta$. Futhermore, for any algorithm
and any class $C$ of Vapnik-Chervonenkis dimension $d \ge 2$ and any $\epsilon
\in (0,1)$ and $\delta \in (0,1)$ there exists a distribution over the domain
and a concept which requires at least $\Omega \left(\frac{d +
\log(1/\delta)}{\epsilon}\right)$ examples to $\epsilon$-learn with probability
at least $1 - \delta$; see~\cite[Theorem 5.3]{Anthony-Bartlett-1999} or
\cite{Blumer-Ehrenfeucht-Haussler-Warmuth-1989,
Ehrenfeucht-Haussler-Kearns-Valiant-1989}.

We construct a family of probability distributions $\P_n$ over $\{0,1\}^n$ such that:
\begin{itemize}
\item For any $P \in \P_n$, any $\epsilon > 0$ there exists an algorithm such
that for any $\delta > 0$ the algorithm $\epsilon$-learns any target function $c
\in C_n$ from $O \left( \frac{\log(1/\delta)}{\epsilon^2} \right)$ examples with
probability at least $1-\delta$.

\item For any algorithm, there exists
a probability distribution $P \in \P_n$ and a target function $c \in C_n$
such that for any $\delta \in (0,\frac{1}{2})$, the algorithm requires
at least $\Omega(\frac{\log(1/\delta) + \log n}{\log \log n})$ examples to
$\frac{1}{4}$-learn the target function with probability at least $1-\delta$.
\end{itemize}

The family $\P_n$ consists of $n! \cdot 2^{n-3}$ probability distributions over
$\{0,1\}^n$. For any permutation $\sigma \in S_n$ and any bit string $b = (b_4,
b_5, \dots, b_n) \in \{0,1\}^{n-3}$, we define a probability distribution
$P_{\sigma,b}$ over $\{0,1\}^n$. To describe the distribution, let $X = (X_1,
X_2, \dots, X_n)$ be a random vector drawn from it. First, $\Pr_{X \sim P_{\sigma,b}}[X = x] =
\prod_{i=1}^n \Pr_{X \sim P_{\sigma,b}}[X_i = x_i]$ for any binary vector $(x_1, x_2, \dots, x_n) \in
\{0,1\}^n$. In other words, $P_{\sigma,b}$ is a product distribution.
Second, we describe the marginal distributions of each coordinate. Let
$$
p_i = \frac{1}{\log_2(1 + i)} \qquad \qquad \text{for $i=1,2,\dots$}
$$
The marginal distributions of $X_{\sigma(1)}$, $X_{\sigma(2)}$,
and $X_{\sigma(3)}$ are
\begin{align*}
\Pr[X_{\sigma(1)} = 1] & = 1 \; , \\
\Pr[X_{\sigma(2)} = 1] & = 0 \; , \\
\Pr[X_{\sigma(3)} = 1] & = \tfrac{1}{2} \; .
\end{align*}
For $i=4,5,\dots,n$, the marginal distributions of $X_{\sigma(i)}$ is
\begin{align*}
\Pr[X_{\sigma(i)} = 1] = p_i^{b_i} (1 - p_i)^{b_i} =
\begin{cases}
p_i & \text{if $b_i = 1$,} \\
1 - p_i & \text{if $b_i = 0$.}
\end{cases}
\end{align*}

\begin{lemma}[Fixed distribution learning]
For any $\epsilon > 0$ and any distribution from $P \in \P_n$,
there exists an algorithm such that for any $\delta > 0$
the algorithm $\epsilon$-learns any target function
from $C_n$ from $O \left( \frac{\log(1/\delta)}{\epsilon^2} \right)$
examples with probability at least $1 - \delta$.
\end{lemma}

\begin{proof}
The proof relies on a result of \cite{Benedek-Itai-1991} for learning under
``fixed'' distributions. If $P$ is any probability distribution on some domain,
then $d_P(f,g) = \Pr_{X \sim P}[f(X) \neq g(X)]$ is a pseudo-metric on the set of all
$\{0,1\}$-valued functions on the domain. \cite{Benedek-Itai-1991} proved that
if a $C$ class of $\{0,1\}$-functions has an $\frac{\epsilon}{2}$-cover of size
$K$, then any target from $C$ is $\epsilon$-learnable from $O
\left( \frac{\log K + \log (1/\delta)}{\epsilon}\right)$ examples
with probability at least $1-\delta$.

We show that for any distribution in $\P_n$ the class $C_n$ has an $\frac{\epsilon}{2}$-cover of size
$K = \max\{3, \lceil 2^{2/\epsilon}\rceil  - 2 \}$. This will imply the lemma.

Consider distribution $P_{\sigma,b} \in \P_n$ for some $\sigma \in S_n$ and $b
\in \{0,1\}^{n-1}$. We claim that $C_{\epsilon/2} = \{ c_{\sigma(1)},
c_{\sigma(2)}, \dots, c_{\sigma(K)}\}$ is an $\frac{\epsilon}{2}$-cover of
$C_n$. That is, we claim that for any $c \in C_n$, there exists $c' \in
C_{\epsilon/2}$ such that $d_{P_{\sigma,b}}(c,c') \le \epsilon/2$.

Consider $c_{\sigma(i)}$ for any $i \in \{1,2,\dots,n\}$. If $i \le K$
then $c_\sigma(i)$ lies in $C_{\epsilon/2}$ and thus, trivially, $d_{P_{\sigma,b}}(c_{\sigma(i)},c_{\sigma(i)}) = 0$.
Otherwise, if  $i \ge K + 1$, we consider two cases. If $b_i = 1$ then
\begin{align*}
d_{P_{\sigma,b}}(c_{\sigma(i)},c_{\sigma(2)})
= \Pr_{X \sim P_{\sigma,b}}[X_{\sigma(i)} \neq X_{\sigma(2)} ]
= \Pr_{X \sim P_{\sigma,b}}[X_{\sigma(i)} = 1 ]
= p_i \le \frac{1}{\log_2(2 + K)}
\le \epsilon/2 \; .
\end{align*}
If $b_i = 0$ then
\begin{align*}
d_{P_{\sigma,b}}(c_{\sigma(i)},c_{\sigma(1)})
= \Pr_{X \sim P_{\sigma,b}}[X_{\sigma(i)} \neq X_{\sigma(1)} ]
= \Pr_{X \sim P_{\sigma,b}}[X_{\sigma(i)} = 0 ]
= p_i \le \frac{1}{\log_2(2 + K)}
\le \epsilon/2 \; .
\end{align*}
Here we used that if $X \sim P_{\sigma,b}$
then $X_{\sigma(1)} = 1$ and $X_{\sigma(2)} = 0$ with probability one.
\end{proof}



\section{OLD VERSION: Realizable Case: Projections}

For any $\epsilon \in (0,\frac{1}{2})$,
we construct a family of probability distributions $\P_{n,\epsilon}$ over $\{0,1\}^n$ such that:
\begin{itemize}
\item For any $P \in \P_{n,\epsilon}$ there exists an algorithm that $(4\epsilon)$-learns
any target function $c \in C_n$ from $O \left( \frac{\log(1/\delta)}{\epsilon} \right)$ examples
with probability at least $1-\delta$.
\item For any algorithm, there exists
a probability distribution $P \in \P_{n,\epsilon}$ and a target function $c \in C_n$
such that for any $\delta \in (0,\frac{1}{2})$, the algorithm requires
at least $\Omega(\log(1/\delta) + \log n)$ examples to $(\epsilon/2)$-learn
the target function with probability at least $1-\delta$.
\end{itemize}

The family $\P_{n,\epsilon}$ consists of $n$ probability distributions $P_1, P_2, \dots, P_n$.
If $X = (X_1, X_2, \dots, X_n)$ is a sample drawn from $P_i$ then $X_1, X_2, \dots, X_n$
are independent Bernoulli random variables such that
$$
\Pr_{X \sim P_i}[X_j = 1] =
\begin{cases}
\frac{1}{2} & \text{if $i = j$,} \\
\epsilon & \text{if $i \neq j$.} \\
\end{cases}
$$

\begin{lemma}[Fixed distribution learning]
For any $\epsilon \in (0,\frac{1}{2})$ and any distribution $P \in
\P_{n,\epsilon}$ there exists an algorithm that $(4\epsilon)$-learns any target
function $c \in C_n$ from $O \left( \frac{\log(1/\delta)}{\epsilon} \right)$
examples with probability at least $1 - \delta$.
\end{lemma}

\begin{proof}
The proof relies on a result of \cite{Benedek-Itai-1991} that for any ``fixed''
distribution $P$, any $\epsilon \in (0,1)$, and any concept class $C$, any
target function from the concept class is $\epsilon$-learnable from $O \left(
\frac{\log |C_{\epsilon/2}| + \log (1/\delta)}{\epsilon}\right)$ examples with
probability at least $1-\delta$. Here, $C_{\epsilon/2}$ is any
$(\epsilon/2)$-cover of $C$ with respect to the pseudometric $d_P(c,c') = \Pr_{x
\sim P}[c(x) \neq c'(x)]$.

We show that any distribution in $\P_{n,\epsilon}$ has a $(2\epsilon)$-cover of
size $2$. Consider distribution $P_i \in \P_{n,\epsilon}$ for some $i$. Choose
an arbitrary $j \in \{1,2,\dots,n\} \setminus \{i\}$. We claim that $\{c_i,
c_j\}$ is an $(2\epsilon)$-cover of $C_n$. That is, we claim that for any $k \in
\{1,2,\dots,n\}$, either $d_{P_i}(c_k,c_i) < 2\epsilon$ or $d_{P_i}(c_k,c_j) <
2\epsilon$.

Indeed, if $k \in \{i,j\}$, trivially $d_{P_i}(c_k,c_i) = 0$ or $d_{P_i}(c_k,c_j) = 0$.
Otherwise, if  $k \in \{1,2,\dots,n\} \setminus \{i,j\}$,
\begin{align*}
d_{P_i}(c_k, c_j)
& = \Pr_{X \sim P_i}[c_k(X) \neq c_j(X)] \\
& = \Pr_{X \sim P_i}[X_k \neq X_j] \\
& = \Pr_{X \sim P_i}[X_k = 1 \wedge X_j = 0] + \Pr_{X \sim P_i}[X_k = 0 \wedge X_j = 1] \\
& = p_k(1 - p_j) + (1 - p_k) p_j \\
& = 2 \epsilon (1-\epsilon) \\
& < 2 \epsilon \; .
\end{align*}
\end{proof}

\begin{lemma}
\label{lemma:projection-distances}
For any $\epsilon \in (0,\frac{1}{2})$,
any two distict probability distributions $P_i,P_j \in P_{n,\epsilon}$
and any function $h:\{0,1\}^n \to \{0,1\}$,
$$
\Pr_{X \sim P_i}[h(X) \neq X_i] + \Pr_{X \sim P_j}[h(X) \neq X_j] \ge \epsilon \; .
$$
\end{lemma}

\begin{proof}
For any $k \in \{1,2,\dots,n\} \setminus \{i,j\}$, $\Pr_{X \sim P_i}[X_k = 1] = \Pr_{X \sim P_j}[X_k = 1]$.
Hence, for any $x \in \{0,1\}^n$ such that $x_i \neq x_j$,
\begin{align*}
& \frac{\displaystyle \min \left\{ \Pr_{X \sim P_i}[X = x], \ \Pr_{X \sim P_j}[X = x] \right\}}{\displaystyle \Pr_{X \sim P_i}[X = x] + \Pr_{X \sim P_j}[X = x]} \\
& =  \frac{ \displaystyle
\left( \prod_{k \in \{1,2,\dots,n\} \setminus \{i,j\}} \Pr_{X \sim P_i}[X_k = x_k] \right) \min \left\{ \Pr_{X \sim P_i}[X_i = x_i] \Pr_{X \sim P_i}[X_j = x_j], \Pr_{X \sim P_j}[X_i = x_i] \Pr_{X \sim P_j}[X_j = x_j]  \right\}
}{ \displaystyle
\left( \prod_{k \in \{1,2,\dots,n\} \setminus \{i,j\}} \Pr_{X \sim P_i}[X_k = x_k] \right) \left( \Pr_{X \sim P_i}[X_i = x_i] \Pr_{X \sim P_i}[X_j = x_j] + \Pr_{X \sim P_j}[X_i = x_i] \Pr_{X \sim P_j}[X_j = x_j]  \right)
} \\
& = \frac{ \displaystyle \min \left\{ \Pr_{X \sim P_i}[X_i = x_i] \Pr_{X \sim P_i}[X_j = x_j], \Pr_{X \sim P_j}[X_i = x_i] \Pr_{X \sim P_j}[X_j = x_j]  \right\}
}{ \displaystyle \Pr_{X \sim P_i}[X_i = x_i] \Pr_{X \sim P_i}[X_j = x_j] + \Pr_{X \sim P_j}[X_i = x_i] \Pr_{X \sim P_j}[X_j = x_j] } \\
& = \frac{ \displaystyle \min \left\{ \frac{1}{2} \epsilon, \ \frac{1}{2} (1-\epsilon) \right\}
}{ \displaystyle \frac{1}{2} \epsilon + \frac{1}{2} (1-\epsilon) } = \frac{\epsilon/2}{1/2} = \epsilon \; .
\end{align*}
Therefore,
\begin{align*}
& \Pr_{X \sim P_i}[h(X) \neq X_i] + \Pr_{X \sim P_j}[h(X) \neq X_j] \\
& \qquad = \sum_{x \in \{0,1\}^n} \Pr_{X \sim P_i}[X = x \wedge h(x) \neq x_i] + \Pr_{X \sim P_j}[X = x \wedge h(x) \neq x_j] \\
& \qquad \ge \sum_{\substack{x \in \{0,1\}^n \\ x_i \neq x_j}} \Pr_{X \sim P_i}[X = x \wedge h(x) \neq x_i] + \Pr_{X \sim P_j}[X = x \wedge h(x) \neq x_j] \\
& \qquad \ge \sum_{\substack{x \in \{0,1\}^n \\ x_i \neq x_j}} \min \left\{ \Pr_{X \sim P_i}[X = x], \ \Pr_{X \sim P_j}[X = x] \right\} \\
& \qquad = \sum_{\substack{x \in \{0,1\}^n \\ x_i \neq x_j}} \epsilon \left( \Pr_{X \sim P_i}[X = x] + \Pr_{X \sim P_j}[X = x] \right) \\
& \qquad = \epsilon \left( \Pr_{X \sim P_i}[X_i \neq X_j] + \Pr_{X \sim P_j}[X_i \neq X_j] \right) \\
& \qquad = \epsilon \left( \frac{1}{2} + \frac{1}{2} \right) \; .
\end{align*}
The first inequality is trivial. The second inequality follows from the fact that
if $x_i \neq x_j$ then $h(x)$ must be equal to either $x_i$ or $x_j$.
\end{proof}

\begin{corollary}
\label{corollary:projection-distances}
For any $\epsilon \in (0,\frac{1}{2})$ and any function $h:\{0,1\}^n \to \{0,1\}$,
for at least $n-1$ out of $n$ possible values of $i \in \{1,2,\dots,n\}$,
$$
\Pr_{X \sim P_i}[h(X) \neq X_i] \ge \epsilon/2 \; .
$$
\end{corollary}

\begin{proof}
By contradiction, suppose there exists a pair $i,j \in \{1,2,\dots,n\}$, $i \neq j$, such that
$$
\Pr_{X \sim P_i}[h(X) \neq X_i] < \epsilon/2 \qquad \text{and} \qquad \Pr_{X \sim P_j}[h(X) \neq X_j] < \epsilon/2 \; .
$$
The existence of such pair violates Lemma~\ref{lemma:projection-distances}.
\end{proof}


\begin{lemma}[Learning without knowledge of the distribution]
For any algorithm $A$ and any $\epsilon \in (0,\frac{1}{2})$,
there exists a distribution $P \in \P_{n,\epsilon}$
and a target concept $c \in C_n$ that require at least $\Omega(\log(1/\delta) + \log n)$
examples to $\frac{\epsilon}{2}$-learn with probability at least $1 - \delta$.
\end{lemma}

\begin{proof}
Fix the sample size $m$ to be any integer between $0$ and $\log(1/\delta) + \frac{1}{2} \log n$.
Let $A$ be any (possibly improper) learning algorithm. We can formalize
it is as a function $A:\bigcup_{k=0}^\infty (\{0,1\}^n \times \{0,1\})^k \to
\{0,1\}^{\{0,1\}^n}$.

We demostrate the existence of a pair $(P,c) \in \P_{n,\epsilon} \times C_n$ by
probabilistic method. Let $\sigma$ be uniformly random permutation over
$\{1,2,\dots,n\}$. Let $I = \sigma(1)$; note that $I$ is uniformly distributed
over $\{1,2,\dots,n\}$. We choose the pair $(P,c)$ to be the (random) pair
$(P_I,c_I)$.

Let $X_1, X_2, \dots, X_m$ be an i.i.d. sample from $P_I$ and
let $Y_1 = c_I(X_1), Y_2 = c_I(X_2), \dots, Y_m = c_I(X_m)$ be the target labels.
We denote by $S = ((X_1, Y_1), (X_2, Y_2), \dots (X_m, Y_m))$ the input examples.
We will show that,
\begin{equation}
\label{equation:expected-failure-probability}
\Exp \left[ \Pr \left[\err_{P_I,c_I}(A(S)) \ge \epsilon/2 \ \middle| \ I \, \right] \right] > \delta \; .
\end{equation}
The inequality \eqref{equation:expected-failure-probability} will imply that for at least one $i \in \{1,2,\dots,n\}$,
$$
\Pr \left[\err_{P_I,c_I}(A(S)) \ge \epsilon/2 \ \middle| \ I = i \right] > \delta \; .
$$
In other words, on $(P_i, c_i)$ the algorithm $A$ fails to
$\epsilon$-learn with probability at least $\delta$.

To prove \eqref{equation:expected-failure-probability}, we first introduce
necessary notation. Let $M$ be $m \times n$ binary matrix $M$ with entries
$M_{i,j} = X_{i,\sigma(j)}$. Note that
$$
\Pr[M_{i,j} = 1] =
\begin{cases}
\frac{1}{2} & \text{if $j = 1$,} \\
\epsilon & \text{if $j \ge 2$.} \\
\end{cases}
$$
Also note that the entries $M_{i,j}$ and $\sigma$ are all mutually independent. Let $C_1, C_2,
\dots, C_n$ be the columns of $M$. Let $J = \{ j ~|~ 1 \le j \le n, \ C_1 = C_j \}$
be the set of columns matching column $C_1$. Note that $1 \in J$ with probability one.

Consider the set of permutations $\{ \pi \in S_n ~|~ \forall i \not \in J, \
\pi(i) = i\}$. Given $J$, we define $\alpha$ to be a permutation chosen
uniformly at random from this set. In other words, $\alpha$ randomly permutes
elements of $J$ and keeps each element of $\{1,2,\dots,n\} \setminus J$ at its
original position.

Note that $\sigma$ and $\alpha$ are independent; this is due the fact that
$\alpha$ is a function of $J$ which in turn is a function of $M$. Somewhat less
obviously, $\sigma \circ \alpha$ and $M$ are independent; this is due to the
fact that conditioned on $M$, $\sigma \circ \alpha$ is a uniformly random
permutation. Therefore, trivially, $(M,\sigma)$ and $(M, \sigma \circ \alpha)$
have the same joint distribution.

Futhermore, $X_{i,\sigma(j)} = X_{i,\sigma(\alpha(j))} = M_{i,j}$ for all
$i,j$, since $\alpha$ permutes only identical columns of $M$. Hence,
$(S,\sigma)$ and $(S,\sigma \circ \alpha)$ have the same joint distribution.
\begin{align*}
\Exp \left[ \Pr \left[\err_{P_I,c_I}(A(S)) \ge \epsilon/2 \ \middle| \ I \, \right] \right]
& = \Exp \left[ \Exp \left[ \indicator{ \err_{P_I,c_I}(A(S)) \ge \epsilon/2} \ \middle| \ I \, \right] \right] \\
& = \Exp \left[ \indicator{ \err_{P_I,c_I}(A(S)) \ge \epsilon/2} \right] \\
& = \Pr \left[ \err_{P_I,c_I}(A(S)) \ge \epsilon/2 \right] \\
& = \Pr \left[ \err_{P_{\sigma(1)},c_{\sigma(1)}}(A(S)) \ge \epsilon/2 \right] \\
& = \Pr \left[ \err_{P_{\sigma(\alpha(1))},c_{\sigma(\alpha(1))}}(A(S)) \ge \epsilon/2 \right] \; .
\end{align*}

For any subset $K \subseteq \{1,2,3,\dots,n\}$, let $L(K)$ bet set all of all $m
\times n$ binary matrices such that $K$ is the set of indices of columns equal
to the first column. Thus,
\begin{align*}
& \Pr \left[ \err_{P_{\sigma(\alpha(1))},c_{\sigma(\alpha(1))}}(A(S)) \ge \epsilon/2 \right] \\
& \ge \Pr \left[ |J| \ge 2 \wedge \err_{P_{\sigma(\alpha(1))},c_{\sigma(\alpha(1))}}(A(S)) \ge \epsilon/2 \right] \\
& = \sum_{\substack{K \subseteq \{1,2,3,\dots,n\} \\ |K| \ge 2}} \Pr \left[ J = K \wedge \err_{P_{\sigma(\alpha(1))},c_{\sigma(\alpha(1))}}(A(S)) \ge \epsilon/2 \right] \\
& = \sum_{\substack{K \subseteq \{1,2,3,\dots,n\} \\ |K| \ge 2}} \sum_{Q \in L(K)} \Pr \left[ M = Q \wedge \err_{P_{\sigma(\alpha(1))},c_{\sigma(\alpha(1))}}(A(S)) \ge \epsilon/2 \right] \\
& = \sum_{\substack{K \subseteq \{1,2,3,\dots,n\} \\ |K| \ge 2}} \sum_{Q \in L(K)} \sum_{\pi \in S_n} \Pr \left[ M = Q \wedge \sigma = \pi \wedge \err_{P_{\sigma(\alpha(1))},c_{\sigma(\alpha(1))}}(A(S)) \ge \epsilon/2 \right] \\
& = \sum_{\substack{K \subseteq \{1,2,3,\dots,n\} \\ |K| \ge 2}} \sum_{Q \in L(K)} \sum_{\pi \in S_n} \Pr \left[ M = Q \wedge \sigma = \pi \right] \Pr \left[ \err_{P_{\sigma(\alpha(1))},c_{\sigma(\alpha(1))}}(A(S)) \ge \epsilon/2 ~\middle|~ M = Q \wedge \pi = \sigma \right] \; .
\end{align*}
Given $M$ and $\sigma$, the values of $S$, $J$ and $A(S)$ are determined.
Let $h = A(S)$. By Corollary~\ref{corollary:projection-distances}, for at least $|J| - 1$
indices $i \in J$,
$$
\err_{P_i,c_i}(h) = \Pr_{X \sim P_i} \left[ h(X) \neq  c_i(X) \right] \ge \epsilon/2 \; .
$$
Conditioned on $M$ and $\sigma$, the index $\sigma(\alpha(1))$ is uniformly distributed over the set $J$. Hence, if $Q \in L(K)$ and $|K| \ge 2$,
$$
\Pr \left[ \err_{P_{\sigma(\alpha(1))},c_{\sigma(\alpha(1))}}(A(S)) \ge \epsilon/2 ~\middle|~ M = Q \wedge \pi = \sigma \right] \ge \frac{|K|-1}{|K|} \ge \frac{1}{2} \; .
$$
We can conclude that
\begin{align*}
\Exp \left[ \Pr \left[\err_{P_I,c_I}(A(S)) \ge \epsilon/2 \ \middle| \ I \, \right] \right]
& \ge \frac{1}{2} \sum_{\substack{K \subseteq \{1,2,3,\dots,n\} \\ |K| \ge 2}} \sum_{Q \in L(K)} \sum_{\pi \in S_n} \Pr \left[ M = Q \wedge \sigma = \pi \right] \\
& = \frac{1}{2} \Pr \left[ |J| \ge 2 \right] \; .
\end{align*}
It remains to show that $\Pr \left[ |J| \ge 2 \right] \ge 2 \delta$.
Recall that $J$ is the subset of columns $C_1, C_2, \dots, C_n$ that are equal to $C_1$.
Using Bonferroni's inequality, we have
\begin{align*}
\Pr\left[ |J| \ge 2 \right]
& = \Pr\left[ \bigvee_{j=2}^n C_j = C_1 \right] \\
& \ge \sum_{i=2}^n \Pr\left[ C_j = C_1 \right] - \sum_{2 \le  j < k \le n} \Pr\left[ C_j = C_k = C_1 \right] \\
& = \sum_{i=2}^n \left( \prod_{j=1}^m \Pr\left[ M_{i,j} = M_{i,1} \right] \right) - \sum_{2 \le j < k \le n} \left( \prod_{j=1}^m \Pr\left[ M_{i,j} = M_{i,k} = M_{i,1} \right] \right) \\
& = (n-1) \left( \frac{1}{2} \right)^m - \binom{n-1}{2} \left( \frac{1}{2} \epsilon^2 + \frac{1}{2} (1-\epsilon)^2 \right)^m \\
& = (n-1) \left( \frac{1}{2} \right)^m \left(1 - \frac{n-2}{2}(1 - 2\epsilon + 2\epsilon^2)^m \right) \\
& \ge (n-1) \left( \frac{1}{2} \right)^m \left(1 - \frac{n-2}{2}(1 - \epsilon)^m \right) \\
& \ge (n-1) \left( \frac{1}{2} \right)^m \left(1 - \frac{n-2}{2} e^{-m\epsilon} \right) \\
\end{align*}
\end{proof}

\section{Realizable Case: Monotone Disjuctions}

Consider the class $C_n$ of monotone disjuctions over $D_n = \{0,1\}^n$.
There are $2^n$ functions in $C_n$. For each subset $I \subseteq \{1,2,\dots,n\}$
there is monotone disjuction $c_I:D_n \to \{0,1\}$ defined by
$$
c_I(x) = \bigvee_{i \in I} x_i \; .
$$
We define $c_\emptyset$ to be constant zero function.

We consider the same family of distributions $\P_n$ as in the previous section.
Let $P = P_{Identity} \in \P_n$. We compute the distance $d_P(c_I, c_J)$
between any two monotone disjuctions. For any subset
$K \subseteq \{1,2,\dots,n\}$, let $A_K$ be the event that all for all $i \in K$, $x_i = 0$.
\begin{align*}
d_P(c_I, c_J)
& = \Pr_{x \sim P}[c_I(x) \neq c_J(x)] \\
& = \Pr_{x \sim P}[A_{I \cap J} \wedge ((A_{I \setminus J} \wedge \overline{A_{J \setminus I}}) \vee (\overline{A_{I \setminus J}} \wedge A_{J \setminus I} )) ] \\
& = \Pr[A_{I \cap J}] \cdot \left\{ \Pr[A_{I \setminus J}] (1 - \Pr[A_{J \setminus I}]) + (1 - \Pr[A_{I \setminus J}]) \Pr[A_{J \setminus I}] \right\} \\
& = \Pr[A_{I \cap J}] \cdot \left\{ \Pr[A_{I \setminus J}] + \Pr[A_{J \setminus I}] - 2 \Pr[A_{I \setminus J}] \Pr[A_{J \setminus I}] \right\} \\
& = \Pr[A_I] + \Pr[A_J] - 2 \Pr[A_{I \cup J}] \\
& = \prod_{i \in I} (1 - p_i) + \prod_{j \in J} (1 - p_j) - 2 \prod_{k \in I \cup J} (1 - p_k) \; . \\
\end{align*}

\section{Agnostic Model}

Let $C$ be a concept class over a domain $X$. Let $P$ be any distribution over
$X$. We define the ``disagreement'' pseudometric $d_P(c,c') = \Pr_{(x,y) \sim
P}[c(x) = c'(x)]$ on $C$. We denote by $N(C,P,\epsilon,)$ be the size of the
smallest $\epsilon$-cover of $C$ with respect to $d_P$.

Let $R(c) = \Pr_{(x,y) \sim P}[c(x) \neq y]$ be the risk.
For an i.i.d. sample $(X_1, Y_1), \dots, (X_n, Y_n)$ be an i.i.d. sample
from $P$, we define the empirical risk
$$
R_n(c) = \sum_{i=1}^n \indicator{c(X_i) \neq Y_i} \; .
$$
Finally, let $c_n = \argmin_{c \in C} R_n(c)$.

\bibliography{biblio}
\bibliographystyle{plainnat}

\end{document}
