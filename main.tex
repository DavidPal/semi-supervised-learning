\documentclass[10pt]{article}

\usepackage{fullpage}
\usepackage{natbib}
\usepackage{amssymb,amsthm,amsmath}

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}

\renewcommand{\P}{\mathcal{P}}
\newcommand{\indicator}[1]{\mathbf{1}\left[{#1}\right]}
\newcommand{\x}{\mathbf{x}}

\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\Exp}{\mathbf{E}}

\title{Semi-supervised learning}
\author{Alexander Golovnev \and D\'avid P\'al}

\begin{document}

\maketitle

\section{Realizable Case: Projections}

Let $C_n$ be the class of projections over the domain $\{0,1\}^n$. The concept
class $C_n$ consists of $n$ functions $c_1, c_2, \dots, c_n$ from $\{0,1\}^n$ to
$\{0,1\}$. For any $i \in \{1,2,\dots,n\}$, the function $c_i$ is defined by
$c_i((x_1, x_2, \dots, x_n)) = x_i$. The Vapnik-Chervonenkis dimension of $C_n$
is $d = \lfloor \log_2 n \rfloor$; see
Proposition~\ref{proposition:vc-dimension-projections} below.

\begin{proposition}
\label{proposition:vc-dimension-projections}
Vapnik-Chervonenkis dimension of $C_n$ is $\lfloor \log_2 n \rfloor$.
\end{proposition}

\begin{proof}
Vapnik-Chervonenkis dimension is the size of the largest shattered set; let us
denote it by $d$. Let $S$ be a shattered set of size $d$. Then, there must be at
least $2^d$ distinct functions in $C_n$. Hence, $d \le \log_2 |C_n| =
\log_2 n$. Since $d$ is an integer, we conclude that $d \le \lfloor \log_2 n
\rfloor$.

On the other hand, we construct a shattered set of size $\lfloor \log_2 n
\rfloor$. The set will consists of points $x_1, x_2, \dots, x_{\lfloor \log_2 n
\rfloor} \in \{0,1\}^n$. For any $i \in \{1,2,\dots,\lfloor \log_2 n \rfloor\}$
and any $j \in \{1,2,\dots,n\}$, we define $x_{i,j}$ to be the $i$-th bit of the
number $j-1$ in its binary representation. (The bit at position $i=1$ is the
least significant bit.) It is not hard to see that for any $v \in
\{0,1\}^{\lfloor \log_2 n \rfloor}$, there exists $c \in C_n$ such that $v =
(c(x_1), c(x_2), \dots, c(x_{\lfloor \log_2 n \rfloor}))$. Indeed, given $v$,
let $k \in \{0,1,\dots,2^{\lfloor \log_2 n \rfloor} - 1\}$ be the number with
binary representation $v$, then we can take $c = c_{k+1}$.
\end{proof}

\cite{Hanneke-2016} showed that for any class $C$ of Vapnik-Chervonenkis
dimension $d$ there exists an algorithm that $\epsilon$-learns any target function
under any domain distribution  from $O\left(\frac{d + \log(1/\delta)}{\epsilon}\right)$
examples with probability at least $1-\delta$. Futhermore, for any algorithm
and any class $C$ of Vapnik-Chervonenkis dimension $d \ge 2$ and any $\epsilon
\in (0,1)$ and $\delta \in (0,1)$ there exists a distribution over the domain
and a concept which requires at least $\Omega \left(\frac{d +
\log(1/\delta)}{\epsilon}\right)$ examples to $\epsilon$-learn with probability
at least $1 - \delta$; see~\cite[Theorem 5.3]{Anthony-Bartlett-1999} or
\cite{Blumer-Ehrenfeucht-Haussler-Warmuth-1989,
Ehrenfeucht-Haussler-Kearns-Valiant-1989}.

We construct a family of probability distributions $\P_n$ over $\{0,1\}^n$ such that:
\begin{itemize}
\item For any $P \in \P_n$, any $\epsilon > 0$ there exists an algorithm such
that for any $\delta > 0$ the algorithm $\epsilon$-learns any target function $c
\in C_n$ from $O \left( \frac{\log(1/\delta)}{\epsilon^2} \right)$ examples with
probability at least $1-\delta$.

\item For any algorithm, there exists
a probability distribution $P \in \P_n$ and a target function $c \in C_n$
such that for any $\delta \in (0,\frac{1}{2})$, the algorithm requires
at least $\Omega(\frac{\log(1/\delta) + \log n}{\log \log n})$ examples to
$\frac{1}{4}$-learn the target function with probability at least $1-\delta$.
\end{itemize}

The family $\P_n$ consists of $n! \cdot 2^{n-1}$ probability distributions over
$\{0,1\}^n$. For any permutation $\sigma \in S_n$ and any bit string $b = (b_1,
b_2,\dots, b_n) \in \{0,1\}^n$, we define a probability distribution
$P_{\sigma,b}$ over $\{0,1\}^n$. To describe the distribution, let $X = (X_1,
X_2, \dots, X_n)$ be a random vector drawn from it. First, $\Pr_{X \sim
P_{\sigma,b}}[X = x] = \prod_{i=1}^n \Pr_{X \sim P_{\sigma,b}}[X_i = x_i]$ for
any binary vector $(x_1, x_2, \dots, x_n) \in \{0,1\}^n$. In other words,
$P_{\sigma,b}$ is a product distribution. Second, we describe the marginal
distributions of each coordinate. Let
$$
p_i = \frac{1}{\log_2(3 + i)} \qquad \qquad \text{for $i=1,2,\dots$}
$$
For $i=1,2,\dots,n$, the marginal distributions of $X_{\sigma(i)}$ is
\begin{align*}
\Pr_{X \sim P_{\sigma,b}}[X_{\sigma(i)} = 1] = p_i^{b_i} (1 - p_i)^{b_i} =
\begin{cases}
p_i & \text{if $b_i = 1$,} \\
1 - p_i & \text{if $b_i = 0$.}
\end{cases}
\end{align*}
We note that $\Pr[X_{\sigma(1)} = 1] = \Pr[X_{\sigma(1)} = 0] = p_1 = 1 - p_1 =
\frac{1}{2}$ regardless of whether $b_1 = 0$ or $b_1 = 1$. Thus, if $b,b' \in \{0,1\}^n$
differ only in the first coordinate, the corresponding distributions $P_{\sigma,b}$
and $P_{\sigma,b'}$ are identical. Hence, $\P_n$ has $n! \cdot 2^n / 2 = n! \cdot 2^{n-1}$
distinct distributions.

\begin{theorem}[Fixed distribution learning]
For any $\epsilon > 0$ and any distribution from $P \in \P_n$,
there exists an algorithm such that for any $\delta > 0$
the algorithm $\epsilon$-learns any target function
from $C_n$ from $O \left( \frac{\log(1/\delta)}{\epsilon^2} \right)$
examples with probability at least $1 - \delta$.
\end{theorem}

\begin{proof}
The proof relies on a result of \cite{Benedek-Itai-1991} for learning under
``fixed'' distributions. If $P$ is any probability distribution on some domain,
then $d_P(f,g) = \Pr_{X \sim P}[f(X) \neq g(X)]$ is a pseudo-metric on the set of all
$\{0,1\}$-valued functions on the domain. \cite{Benedek-Itai-1991} proved that
if a $C$ class of $\{0,1\}$-functions has an $\frac{\epsilon}{2}$-cover of size
$N$, then any target from $C$ is $\epsilon$-learnable from $O
\left( \frac{\log N + \log (1/\delta)}{\epsilon}\right)$ examples
with probability at least $1-\delta$.

We show that for any distribution in $\P_n$ the class $C_n$ has an $\frac{\epsilon}{2}$-cover of size
$N = \max\{3, \lceil 2^{4/\epsilon}\rceil  - 2 \}$. This will imply the lemma.

Consider distribution $P_{\sigma,b} \in \P_n$ for some $\sigma \in S_n$ and $b
\in \{0,1\}^{n-1}$. Let $i$ be the largest index such that $b_i = 1$.
If not such $i$ exists, we define $i=1$. Likewise, let $j$ be the largest
index such that $b_j = 0$. If no such index exists, we define $j=1$.
We claim that $C_{\epsilon/2} = \{ c_{\sigma(i)}, c_{\sigma(j)}, c_{\sigma(1)},
c_{\sigma(2)}, \dots, c_{\sigma(N-2)}\}$ is an $\frac{\epsilon}{2}$-cover of
$C_n$. That is, we claim that for any $c \in C_n$, there exists $c' \in
C_{\epsilon/2}$ such that $d_{P_{\sigma,b}}(c,c') \le \epsilon/2$.

Consider $c_{\sigma(k)}$ for any $k \in \{1,2,\dots,n\}$. If $k \le N - 2$
then $c_\sigma(k)$ lies in $C_{\epsilon/2}$ and thus, trivially, $d_{P_{\sigma,b}}(c_{\sigma(k)},c_{\sigma(k)}) = 0$.
Otherwise, if  $k \ge N - 1$, we consider two cases. If $b_k = 1$ then $i \ge k$ and $b_i = 1$, and
\begin{multline*}
d_{P_{\sigma,b}}(c_{\sigma(i)}, c_{\sigma(k)})
= \Pr_{X \sim P_{\sigma,b}}[X_{\sigma(i)} \neq X_{\sigma(k)} ] \\
= p_i (1 - p_k) + p_k (1 - p_i)
\le p_i + p_k
\le 2 p_k
\le \frac{2}{\log_2(3 + k)}
\le \frac{2}{\log_2(2 + N)}
\le \epsilon/2 \; .
\end{multline*}
If $b_i = 0$ then then $j \ge k$ and $b_j = 0$, and
\begin{multline*}
d_{P_{\sigma,b}}(c_{\sigma(j)}, c_{\sigma(k)})
= \Pr_{X \sim P_{\sigma,b}}[X_{\sigma(j)} \neq X_{\sigma(k)} ] \\
= p_j (1 - p_k) + p_k (1 - p_j)
\le p_j + p_k
\le 2 p_k
\le \frac{2}{\log_2(3 + k)}
\le \frac{2}{\log_2(2 + N)}
\le \epsilon/2 \; .
\end{multline*}
\end{proof}

For any function $h:\{0,1\}^n \to \{0,1\}$, any vector $b \in \{0,1\}^n$,
any permutation $\sigma \in S_n$, any projection $c_i \in C_n$, define
$$
\err_{c_i,P_{\sigma,b}} = \Pr_{X \sim P_{\sigma,b}}[h(X) \neq c_i(X)] \; .
$$
In other words, $\err_{P_{\sigma,b},c_i}$ is
the error of $h$ with respect to the target $c_i$
under the distribution $P_{\sigma,b}$.

A permutation $\pi \in S_n$ is said to be a swap of $i,j$
if $\pi(i) = j$ and $\pi(j) = i$ and for all $k \in \{1,2,\dots,n\} \setminus \{i,j\}$
$\pi(k) = k$.

\begin{lemma}
\label{lemma:projection-distances}
Let $i \in \{2,3,\dots,n\}$. Let $\alpha \in S_n$ be a swap of $1$ and $i$.
Let $a,b \in \{0,1\}^n$ be binary vectors such that $a_i \neq b_i$
and $a_j = b_j$ for all $j \in \{1,2,\dots,n\} \setminus \{i\}$.
Let $h:\{0,1\}^n \to \{0,1\}$ be an arbitary function. Then, for any permutation $\sigma \in S_n$,
$$
\err_{c_{\sigma(1)},P_{\sigma,a}}(h) + \err_{c_{\sigma(1)},P_{\sigma,b}}(h) +
\err_{c_{\sigma(\alpha(1))},P_{\sigma \circ \alpha,b}}(h) + \err_{c_{\sigma(\alpha(1))},P_{\sigma \circ \alpha,b}}(h) \ge 1 \; .
$$
\end{lemma}

\begin{proof}
We claim that for any $x \in \{0,1\}^n$,
\begin{equation}
\label{equation:probability-ratio}
\frac{\displaystyle
\min \left\{ \Pr_{X \sim P_{\sigma,a}}[X = x] + \Pr_{X \sim P_{\sigma,b}}[X = x], \ \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X = x] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X = x] \right\}}{
\displaystyle \Pr_{X \sim P_{\sigma,a}}[X = x] + \Pr_{X \sim P_{\sigma,b}}[X = x] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X = x] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X = x]}
= \frac{1}{2} \; .
\end{equation}
Since $P_{\sigma,a}$,
$P_{\sigma,b}$, $P_{\sigma \circ \alpha,a}$, $P_{\sigma \circ \alpha,b}$
are product distributions and the marginal distributions of $X_{\sigma(k)}$
for any $k \not \in \{1,i\}$ under these four distributions are the same,
after cancelling common terms, we can write the fraction on the left side as $A/B$ where
\begin{multline*}
A =
\min \left\{ \Pr_{X \sim P_{\sigma,a}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}]
+ \Pr_{X \sim P_{\sigma,b}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}],  \right. \\
\left. \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}]
+ \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}]  \right\}
\end{multline*}
and
\begin{multline*}
B =
\Pr_{X \sim P_{\sigma,a}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] + \Pr_{X \sim P_{\sigma,b}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] \\
+ \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] \; .
\end{multline*}
Since $a_i \neq b_i$,
$$
\Pr_{X \sim P_{\sigma,a}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] + \Pr_{X \sim P_{\sigma,b}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] =
\Pr_{X \sim P_{\sigma,a}}[X_{\sigma(1)} = x_{\sigma(1)}] = \frac{1}{2}
$$
and
$$
\Pr_{X \sim P_{\sigma \circ \alpha,a}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] =
\Pr_{X \sim P_{\sigma \circ \alpha,a}}[X_{\sigma(i)} = x_{\sigma(i)}] = \frac{1}{2} \; .
$$

\allowdisplaybreaks
Equipped with \eqref{equation:probability-ratio} we finish the proof of the lemma:
\begin{align*}
& \err_{c_{\sigma(1)},P_{\sigma,a}}(h) + \err_{c_{\sigma(1)},P_{\sigma,b}}(h) + \err_{c_{\sigma(\alpha(1))},P_{\sigma \circ \alpha,b}}(h) + \err_{c_{\sigma(\alpha(1))},P_{\sigma \circ \alpha,b}}(h) \\
& = \err_{c_{\sigma(1)},P_{\sigma,a}}(h) + \err_{c_{\sigma(1)},P_{\sigma,b}}(h) + \err_{c_{\sigma(i)},P_{\sigma \circ \alpha,a}}(h) + \err_{c_{\sigma(i)},P_{\sigma \circ \alpha,b}}(h) \\
& = \Pr_{X \sim P_{\sigma,a}}[h(X) \neq X_{\sigma(1)}] + \Pr_{X \sim P_{\sigma,b}}[h(X) \neq X_{\sigma(1)}] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[h(X) \neq X_{\sigma(i)}] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[h(X) \neq X_{\sigma(i)}] \\
& = \sum_{x \in \{0,1\}^n} \Pr_{X \sim P_{\sigma,a}}[X = x \wedge h(x) \neq x_{\sigma(1)}] + \Pr_{X \sim P_{\sigma,b}}[X = x \wedge h(x) \neq x_{\sigma(1)}] \\
  & \qquad \qquad + \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X = x \wedge h(x) \neq x_{\sigma(i)}] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X = x \wedge h(x) \neq x_{\sigma(i)}] \\
& \ge \sum_{\substack{x \in \{0,1\}^n \\ x_{\sigma(1)} \neq x_{\sigma(i)}}} \Pr_{X \sim P_{\sigma,a}}[X = x \wedge h(x) \neq x_{\sigma(1)}] + \Pr_{X \sim P_{\sigma,b}}[X = x \wedge h(x) \neq x_{\sigma(1)}] \\
& \qquad \qquad + \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X = x \wedge h(x) \neq x_{\sigma(i)}] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X = x \wedge h(x) \neq x_{\sigma(i)}]  \\
& \ge \sum_{\substack{x \in \{0,1\}^n \\ x_{\sigma(1)} \neq x_{\sigma(i)}}} \min \left\{ \Pr_{X \sim P_{\sigma,a}}[X = x] + \Pr_{X \sim P_{\sigma,b}}[X = x], \ \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X = x] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X = x] \right\}
  \displaybreak[4] \\
& = \sum_{\substack{x \in \{0,1\}^n \\ x_{\sigma(1)} \neq x_{\sigma(i)}}} \frac{1}{2} \left(
\Pr_{X \sim P_{\sigma,a}}[X = x] + \Pr_{X \sim P_{\sigma,b}}[X = x] + \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X = x] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X = x] \right) \\
& = \frac{1}{2} \left( \Pr_{X \sim P_{\sigma,a}}[X_{\sigma(1)} \neq X_{\sigma(i)}] + \Pr_{X \sim P_{\sigma,b}}[X_{\sigma(1)} \neq X_{\sigma(i)}]
+ \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X_{\sigma(1)} \neq X_{\sigma(i)}] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X_{\sigma(1)} \neq X_{\sigma(i)}] \right) \\
& = \frac{1}{2} \left( \frac{1}{2} + \frac{1}{2} + \frac{1}{2} + \frac{1}{2} \right) = 1 \; .
\end{align*}
The first inequality is trivial. The second inequality follows from the fact that
if $x_{\sigma(1)} \neq x_{\sigma(i)}$ then $h(x)$ must be equal to either $x_{\sigma(1)}$ or $x_{\sigma(i)}$.
\end{proof}



\begin{theorem}[Learning without knowledge of the distribution]
For any algorithm $A$ there exists a distribution $P \in \P_n$
and a target concept $c \in C_n$ that requires at least $\Omega \left(\frac{\log n + \log(1/\delta)}{\log \log n} \right)$
examples to $\frac{1}{4}$-learn with probability at least $1 - \delta$.
\end{theorem}

\begin{proof}
Fix the number of examples $m$ to be any integer between $0$ and
$\frac{\log(1/\delta) + \frac{1}{2} \log n}{\log \log n}$. Let $A$ be any
(possibly improper) learning algorithm. We can formalize it is as a function
$A:\bigcup_{k=0}^\infty (\{0,1\}^n \times \{0,1\})^k \to \{0,1\}^{\{0,1\}^n}$.

We demostrate the existence of a pair $(P,c) \in \P_n \times C_n$ by
probabilistic method. Let $\sigma$ be uniformly random permutation over
$\{1,2,\dots,n\}$ and let $b = (b_1, b_2, \dots, b_n)$ be a vector chosen
uniformly at random from $\{0,1\}^n$. Let $I = \sigma(1)$; note that $I$ is
uniformly distributed over $\{1,2,\dots,n\}$. We choose the pair $(P,c)$ to be
the (random) pair $(P_{\sigma,b},c_I)$.

Let $X_1, X_2, \dots, X_m$ be an i.i.d. sample from $P_{\sigma,b}$ and
let $Y_1 = c_I(X_1), Y_2 = c_I(X_2), \dots, Y_m = c_I(X_m)$ be the target labels.
We denote by $S = ((X_1, Y_1), (X_2, Y_2), \dots (X_m, Y_m))$ the input examples.
We will show that,
\begin{equation}
\label{equation:expected-failure-probability}
\Exp \left[ \Pr \left[\err_{P_{\sigma,b},c_I}(A(S)) \ge \frac{1}{4} \ \middle| \ \sigma, b \, \right] \right] \ge \delta \; .
\end{equation}
The inequality \eqref{equation:expected-failure-probability} will imply that
there exists $\sigma \in S_n$, $b \in \{0,1\}^n$, and $i \in
\{1,2,\dots,n\}$ such that under the distribution $P_{\sigma,b}$, the
algorithm $A$ fails to $\frac{1}{4}$-learn the target $c_i$ with probability at least $\delta$.

To prove \eqref{equation:expected-failure-probability}, we first introduce
necessary notation. Let $Y = (Y_1, Y_2, \dots, Y_m)$ of labels.
Let $M$ be $m \times n$ binary matrix $M$ with entries
$M_{i,j} = X_{i,\sigma(j)}$. Note that
$$
\Pr[M_{i,j} = 1 ~|~ b_j] =
p_j^{b_j} (1 - p_j)^{b_j} =
\begin{cases}
p_j & \text{if $b_j = 1$,} \\
1 - p_j & \text{if $b_j = 0$.}
\end{cases}
$$
Also note that the entries $M_{i,j}$ and $\sigma$ are all mutually independent.
Let $C_1, C_2, \dots, C_n$ be the columns of $M$. Let $J = \{ j ~|~ 1 \le j \le
n, \ C_j = Y \}$ be the set of columns matching column of labels $Y$. Note that,
with probability one, $C_1 = Y$ and hence $1 \in J$.

Let $R$ be the largest index in $J$. Let $\alpha \in S_n$ be the permutation
that swaps $1$ and $R$ and keeps all the other elements in place. Let $a \in
\{0,1\}^n$ be the vector obtained from $b$ by flipping $R$-th coordinate.
That is, $a_R = 1 - b_R$ and for any other coordinate $j$, $a_j = b_j$.

Note that $\sigma$ and $\alpha$ are independent. Somewhat less obviously,
$\sigma \circ \alpha$ and $M$ are independent and thus $(M,\sigma)$ and $(M,
\sigma \circ \alpha)$ have the same distribution; this is due to the fact that
conditioned on $M$, both $\sigma$ and $\sigma \circ \alpha$ have uniform
distribution over $S_n$. Futhermore, $X_{i,\sigma(j)} = X_{i,\sigma(\alpha(j))} =
M_{i,j}$ for all $i,j$, since $\alpha$ permutes only identical columns of $M$.
Hence, $(S,\sigma)$ and $(S,\sigma \circ \alpha)$ have the same joint
distribution. Therefore,
\begin{align*}
& \Pr \left[ \err_{c_{\sigma(1),P_{\sigma, b}}}(A(S)) \ge \frac{1}{4} \right] \\
& \ge \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, b}}(A(S)) \ge \frac{1}{4} \right] \\
& = \frac{1}{2} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, b}}(A(S)) \ge \frac{1}{4} \right] + \frac{1}{2} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, b}}(A(S)) \ge \frac{1}{4} \right]  \\
& = \frac{1}{4} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, b}}(A(S)) \ge \frac{1}{4} \right] + \frac{1}{4} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, b}}(A(S)) \ge \frac{1}{4} \right] \\
& \qquad + \frac{1}{4} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, b}}(A(S)) \ge \frac{1}{4} \right] + \frac{1}{4} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, b}}(A(S)) \ge \frac{1}{4} \right] \\
& \ge \frac{1}{4} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, b}}(A(S)) \ge \frac{1}{4} \right] + \frac{1}{4} p_n^m  \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, a}}(A(S)) \ge \frac{1}{4} \right] \\
& \qquad + \frac{1}{4} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, b}}(A(S)) \ge \frac{1}{4} \right] + \frac{1}{4} p_n^m \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, a}}(A(S)) \ge \frac{1}{4} \right] \\
& \ge \frac{1}{4} p_n^m \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, b}}(A(S)) \ge \frac{1}{4} \right] + \frac{1}{4} p_n^m  \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, a}}(A(S)) \ge \frac{1}{4} \right] \\
& \qquad + \frac{1}{4} p_n^m \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, b}}(A(S)) \ge \frac{1}{4} \right] + \frac{1}{4} p_n^m \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, a}}(A(S)) \ge \frac{1}{4} \right] \\
& \ge \frac{1}{4} p_n^m \Pr \left[ |J| \ge 2 \wedge \left( \err_{c_{\sigma(1)},P_{\sigma, b}}(A(S)) \ge \frac{1}{4} \vee \err_{c_{\sigma(1)},P_{\sigma, a}}(A(S)) \ge \frac{1}{4} \right. \right. \\
& \qquad \qquad \qquad \left. \left. \vee \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, b}}(A(S)) \ge \frac{1}{4} \vee \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, a}}(A(S)) \ge \frac{1}{4} \right) \right] \\
& = \frac{1}{4} p_n^m \Pr \left[ |J| \ge 2 \right] \; .
\end{align*}
The last equality follows from Lemma~\ref{lemma:projection-distances}.

It remains to show that $\frac{1}{4} p_n^m \Pr \left[ |J| \ge 2 \right] \ge \delta$.
We first show that
$$
\Pr \left[ |J| \ge 2 \right] \ge \frac{1}{2} \; .
$$
Recall that $J$ is the subset of columns $C_1, C_2, \dots, C_n$ that are equal
to $C_1$. Let $x \in \{0,1\}^m$ be any binary vector. Conditioned on the event
$C_1 = x$, the events $C_2 = x$, $C_3 = x$, \dots, $C_n = x$ are independent.
Futhermore, $\Pr[C_j = x | C_1 = x] \ge p_n^m$.
$$
\Pr \left[ \bigwedge_{j=2}^n \{C_j \neq x\} ~\middle|~ C_1 = x \right] = \prod_{j=2}^n \Pr[C_j \neq x | C_1 = x] \le (1 - p_n^m)^{n-1} \le (1 - p_n^m)^n \; .
$$
Summing over all possible choices of $x$, we get
$$
\Pr[|J| \le 1] = \Pr \left[ \bigwedge_{j=2}^n \{C_j \neq C_1\} \right] \le (1 - p_n^m)^n \le e^{-n p_n^m} \; .
$$
To prove that $e^{-n p_n^m} \le \frac{1}{2}$, we show the equivalent statement $\ln \ln e^{n p_n^m} \ge \ln \ln 2$.
Indeed,
$$
\ln \ln e^{n p_n^m} = \ln n + m \ln p_n = \ln n + m \ln \log_2(3+n)
$$
\end{proof}

\section{Realizable Case: Monotone Disjuctions}

Consider the class $C_n$ of monotone disjuctions over $D_n = \{0,1\}^n$.
There are $2^n$ functions in $C_n$. For each subset $I \subseteq \{1,2,\dots,n\}$
there is monotone disjuction $c_I:D_n \to \{0,1\}$ defined by
$$
c_I(x) = \bigvee_{i \in I} x_i \; .
$$
We define $c_\emptyset$ to be constant zero function.

We consider the same family of distributions $\P_n$ as in the previous section.
Let $P = P_{Identity} \in \P_n$. We compute the distance $d_P(c_I, c_J)$
between any two monotone disjuctions. For any subset
$K \subseteq \{1,2,\dots,n\}$, let $A_K$ be the event that all for all $i \in K$, $x_i = 0$.
\begin{align*}
d_P(c_I, c_J)
& = \Pr_{x \sim P}[c_I(x) \neq c_J(x)] \\
& = \Pr_{x \sim P}[A_{I \cap J} \wedge ((A_{I \setminus J} \wedge \overline{A_{J \setminus I}}) \vee (\overline{A_{I \setminus J}} \wedge A_{J \setminus I} )) ] \\
& = \Pr[A_{I \cap J}] \cdot \left\{ \Pr[A_{I \setminus J}] (1 - \Pr[A_{J \setminus I}]) + (1 - \Pr[A_{I \setminus J}]) \Pr[A_{J \setminus I}] \right\} \\
& = \Pr[A_{I \cap J}] \cdot \left\{ \Pr[A_{I \setminus J}] + \Pr[A_{J \setminus I}] - 2 \Pr[A_{I \setminus J}] \Pr[A_{J \setminus I}] \right\} \\
& = \Pr[A_I] + \Pr[A_J] - 2 \Pr[A_{I \cup J}] \\
& = \prod_{i \in I} (1 - p_i) + \prod_{j \in J} (1 - p_j) - 2 \prod_{k \in I \cup J} (1 - p_k) \; . \\
\end{align*}

\section{Agnostic Model}

Let $C$ be a concept class over a domain $X$. Let $P$ be any distribution over
$X$. We define the ``disagreement'' pseudometric $d_P(c,c') = \Pr_{(x,y) \sim
P}[c(x) = c'(x)]$ on $C$. We denote by $N(C,P,\epsilon,)$ be the size of the
smallest $\epsilon$-cover of $C$ with respect to $d_P$.

Let $R(c) = \Pr_{(x,y) \sim P}[c(x) \neq y]$ be the risk.
For an i.i.d. sample $(X_1, Y_1), \dots, (X_n, Y_n)$ be an i.i.d. sample
from $P$, we define the empirical risk
$$
R_n(c) = \sum_{i=1}^n \indicator{c(X_i) \neq Y_i} \; .
$$
Finally, let $c_n = \argmin_{c \in C} R_n(c)$.

\bibliography{biblio}
\bibliographystyle{plainnat}

\end{document}
