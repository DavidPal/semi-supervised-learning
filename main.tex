\documentclass[10pt]{article}

\usepackage{fullpage}
\usepackage{graphics}
\usepackage{natbib}
\usepackage{amssymb,amsthm,amsmath}

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}

\renewcommand{\P}{\mathcal{P}}
\newcommand{\indicator}[1]{\mathbf{1}\left[{#1}\right]}
\newcommand{\x}{\mathbf{x}}

\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\Exp}{\mathbf{E}}


\begin{document}

\title{Semi-supervised learning: The value of unlabeled data}
\author{Alexander Golovnev \and D\'avid P\'al \and Bal\'azs Sz\"or\'enyi}

\maketitle

\begin{abstract}
We show a separation of labeled sample complexity between learning \emph{with}
and \emph{without} the knowledge of the distribution of the unlabeled data. For
the class of projections over the boolean hypercube of dimension $n$, we show a
separation by $\Theta(\log n)$ multiplicative factor.

Learning with the knowledge of the distribution (a.k.a. \emph{fixed-distribution
learning}) can be viewed as an idealized scenario of semi-supervised learning
where the number of unlabeled data points is so great that the unlabeled
distribution is known almost exactly. For this reason, we call the separation
the \emph{value of unlabeled data}.
\end{abstract}


\section{Realizable Case: Projections}

\cite{Hanneke-2016} showed that for any class $C$ of Vapnik-Chervonenkis
dimension $d$ there exists an algorithm that $\epsilon$-learns any target
function under any domain distribution  from $O\left(\frac{d +
\log(1/\delta)}{\epsilon}\right)$ examples with probability at least $1-\delta$.
On the other hand, for any algorithm and any class $C$ of Vapnik-Chervonenkis dimension
$d \ge 2$ and any $\epsilon \in (0,1)$ and $\delta \in (0,1)$ there exists a
distribution over the domain and a concept which requires at least $\Omega
\left(\frac{d + \log(1/\delta)}{\epsilon}\right)$ examples to $\epsilon$-learn
with probability at least $1 - \delta$; see~\cite[Theorem
5.3]{Anthony-Bartlett-1999} or \cite{Blumer-Ehrenfeucht-Haussler-Warmuth-1989,
Ehrenfeucht-Haussler-Kearns-Valiant-1989}.

The proof of the lower bound constructs a distribution that does \emph{not}
depend on the algorithm. (The distribution is a fixed distribution over a
fixed set shattered by $C$.) So even an algorithm that ``knows'' (``is tailored
to'') the distribution requires $\Omega \left(\frac{d +
\log(1/\delta)}{\epsilon}\right)$ to learn.

Let $C_n$ be the class of projections over $\{0,1\}^n$. Vapnik-Chervonenkis
dimension of $C_n$ is $\lfloor \log_2 n \rfloor$. We construct a family of
probability distributions $\P_n$ over $\{0,1\}^n$ such that:

\begin{itemize}
\item For any $P \in \P_n$, any $\epsilon > 0$ there exists an algorithm such
that for any $\delta > 0$ the algorithm $\epsilon$-learns any target function $c
\in C_n$ from $O \left( \frac{\log(1/\delta)}{\epsilon^2} \right)$ examples with
probability at least $1-\delta$.

\item For any algorithm, there exists
a probability distribution $P \in \P_n$ and a target function $c \in C_n$
such that for any $\delta \in (0,\frac{1}{2})$, the algorithm requires
at least $\Omega(\frac{\log(1/\delta) + \log n}{\log \log n})$ examples to
$\frac{1}{4}$-learn the target function with probability at least $1-\delta$.
\end{itemize}
%
The situation for constant $\epsilon$ and $\delta$ is summarized in
Figure~\ref{figure:sample-complexity}.

\begin{figure}
\centering
\includegraphics{figure}
\caption{The graph shows sample complexity of learning a class of projections
$C_n$ under various distributions over the domain $\{0,1\}^n$. We assume that
$\epsilon$ and $\delta$ are constant; e.g. $\epsilon = \delta = 0.1$. The graph
shows three lines. The red horizontal line is the optimal PAC sample complexity for class of
projections, which is $\Theta(VC(C_n)) = \Theta(\log_2 n)$. The green line corresponds to
the optimal sample complexity for fixed-distribution learning, i.e., the learning where algorithm ``knows''
(``is tailored to'') the specific distribution. The green line touches the red
line for certain distributions, but is lower for other distributions.
In particular, for certain distributions the green line is $O(1)$.
The black line is the sample complexity for an arbitrary fixed algorithm
that does \emph{not} receive the distribution as input. (For example, the reader can think of the ERM
algorithm.) Obviously, the black line must lie above the green line. In this paper,
we prove that there exist a distribution where the black line is $\Omega(\log
n)$ times higher than green line.}
\label{figure:sample-complexity}
\end{figure}

\section{Preliminaries}

We denote by $C_n$ the class of projections over the domain $\{0,1\}^n$. The
class $C_n$ consists of $n$ functions $c_1, c_2, \dots, c_n$ from $\{0,1\}^n$ to
$\{0,1\}$. For any $i \in \{1,2,\dots,n\}$, for any $x \in \{0,1\}^n$,
the function $c_i$ is defined as $c_i((x[1], x[2], \dots, x[n])) = x[i]$.
The Vapnik-Chervonenkis dimension of $C_n$ is $d = \lfloor \log_2 n \rfloor$; see
Proposition~\ref{proposition:vc-dimension-projections} below.

\begin{proposition}
\label{proposition:vc-dimension-projections}
Vapnik-Chervonenkis dimension of $C_n$ is $\lfloor \log_2 n \rfloor$.
\end{proposition}

\begin{proof}
Vapnik-Chervonenkis dimension is the size of the largest shattered set; let us
denote it by $d$. Let $S$ be a shattered set of size $d$. Then, there must be at
least $2^d$ distinct functions in $C_n$. Hence, $d \le \log_2 |C_n| =
\log_2 n$. Since $d$ is an integer, we conclude that $d \le \lfloor \log_2 n
\rfloor$.

On the other hand, we construct a shattered set of size $\lfloor \log_2 n
\rfloor$. The set will consists of points $x_1, x_2, \dots, x_{\lfloor \log_2 n
\rfloor} \in \{0,1\}^n$. For any $i \in \{1,2,\dots,\lfloor \log_2 n \rfloor\}$
and any $j \in \{0,1,2,\dots,n-1\}$, we define $x_i[j]$ to be the $i$-th bit of the
in the binary representation of the number $j$. (The bit at position $i=1$ is the
least significant bit.) It is not hard to see that for any $v \in
\{0,1\}^{\lfloor \log_2 n \rfloor}$, there exists $c \in C_n$ such that $v =
(c(x_1), c(x_2), \dots, c(x_{\lfloor \log_2 n \rfloor}))$. Indeed, given $v$,
let $k \in \{0,1,\dots,2^{\lfloor \log_2 n \rfloor} - 1\}$ be the number with
binary representation $v$, then we can take $c = c_{k+1}$.
\end{proof}

The family $\P_n$ consists of $n!$ probability distributions over
$\{0,1\}^n$. For any permutation $\sigma \in S_n$, we define a probability distribution
$P_{\sigma}$ over $\{0,1\}^n$. To describe the distribution $P_\sigma$, let $X = (X_1,
X_2, \dots, X_n)$ be a random vector with this distribution.
The distribution is a product distribution, i.e., $\Pr[X = x] = \prod_{i=1}^n \Pr_{X}[X_i = x[i]]$
for any $x \in \{0,1\}^n$. The marginal
distribution of each coordinate $i$ is
$$
\Pr[X_{\sigma(i)} = 1] = p_i = \frac{1}{\log_2(3 + i)} \; .
$$

\begin{theorem}[Fixed distribution learning]
For any $\epsilon > 0$ and any distribution from $P \in \P_n$,
there exists an algorithm such that for any $\delta > 0$
the algorithm $\epsilon$-learns any target function
from $C_n$ from $O \left( \frac{\log(1/\delta)}{\epsilon^2} \right)$
examples with probability at least $1 - \delta$.
\end{theorem}

\begin{proof}
The proof relies on a result of \cite{Benedek-Itai-1991} for learning under
``fixed'' distributions. If $P$ is any probability distribution on some domain,
then $d_P(f,g) = \Pr_{X \sim P}[f(X) \neq g(X)]$ is a pseudo-metric on the set of all
$\{0,1\}$-valued functions on the domain. \cite{Benedek-Itai-1991} proved that
if a $C$ class of $\{0,1\}$-functions has an $\frac{\epsilon}{2}$-cover of size
$N$, then any target from $C$ is $\epsilon$-learnable from $O
\left( \frac{\log N + \log (1/\delta)}{\epsilon}\right)$ examples
with probability at least $1-\delta$.

We show that for any distribution in $\P_n$ the class $C_n$ has an $\frac{\epsilon}{2}$-cover of size
$N = \max\{3, \lceil 2^{4/\epsilon}\rceil  - 2 \}$. This will imply the lemma.

Consider distribution $P_{\sigma} \in \P_n$ for some $\sigma \in S_n$.
We claim that $C_{\epsilon/2} = \{ c_{\sigma(1)},
c_{\sigma(2)}, \dots, c_{\sigma(N)} \}$ is an $\frac{\epsilon}{2}$-cover of
$C_n$. That is, we claim that for any $c \in C_n$, there exists $c' \in
C_{\epsilon/2}$ such that $d_{P_{\sigma}}(c,c') \le \epsilon/2$.

Any $c \in C_n$ is of the form $c = c_{\sigma(k)}$ for some $k \in \{1,2,\dots,n\}$.
If $k \le N$ then $c_{\sigma(k)}$ lies in $C_{\epsilon/2}$ and thus, trivially, $d_{P_{\sigma}}(c_{\sigma(k)},c_{\sigma(k)}) = 0$.
Otherwise, if  $k \ge N + 1$,
\begin{align*}
d_{P_{\sigma}}(c_{\sigma(N)}, c_{\sigma(k)})
& = \Pr_{X \sim P_{\sigma}}[X_{\sigma(N)} \neq X_{\sigma(k)} ] \\
& = p_N (1 - p_k) + p_k (1 - p_N) \\
& \le p_N + p_k \\
& \le 2 p_N \\
& \le \frac{2}{\log_2(2 + N)} \\
& \le \epsilon/2 \; .
\end{align*}
\end{proof}

For any function $h:\{0,1\}^n \to \{0,1\}$ any permutation $\sigma \in S_n$, any
projection $i \in \{1,2,\dots,n\}$, define
$$
\err_{\sigma,i}(h) = \Pr_{X \sim P_{\sigma}}[h(X) \neq c_i(X)] \; .
$$
In other words, $\err_{\sigma,i}$ is the error of $h$ with respect to the target
$c_i$ under the distribution $P_{\sigma}$.

\begin{theorem}[Learning without knowledge of the distribution]
For any algorithm $A$ there exists a distribution $P \in \P_n$
and a target concept $c \in C_n$ that requires at least $\Omega \left(??? \right)$
labeled examples to $\epsilon$-learn with probability at least $1 - \delta$.
\end{theorem}

\begin{proof}
Fix the number of examples $m$ to be any non-negative less than $\log n$. Let $A$
be any (possibly improper) learning algorithm. We can formalize it is as a
function $A:\bigcup_{k=0}^\infty (\{0,1\}^n \times \{0,1\})^k \to
\{0,1\}^{\{0,1\}^n}$.

We demostrate the existence of a pair $(P,c) \in \P_n \times C_n$ by
probabilistic method. Let $\sigma$ be uniformly random permutation over
$\{1,2,\dots,n\}$. Let $I = \sigma(1)$; note that $I$ is uniformly distributed
over $\{1,2,\dots,n\}$. We will show that if we feed $A$ an i.i.d. sample of
size $m$ from $P_\sigma$ labeled according to $c_I$, with probability at least
$\delta$ the error of the algorithm is at least $\epsilon$. This will imply
the existence of the pair $(P,c)$.

Formally, let $X_1, X_2, \dots, X_m$ be an i.i.d. sample from $P_{\sigma}$ and
let $Y_1 = c_I(X_1), Y_2 = c_I(X_2), \dots, Y_m = c_I(X_m)$ be the target labels.
Let $S = ((X_1, Y_1), (X_2, Y_2), \dots (X_m, Y_m))$ be the input sample.
We will show that
\begin{equation}
\label{equation:failure-probability}
\Pr \left[\err_{\sigma,I}(A(S)) \ge \epsilon \right] \ge \delta \; .
\end{equation}
Since
$$
\Pr \left[\err_{\sigma,I}(A(S)) \ge \epsilon \right] = \Exp\left[ \Pr \left[\err_{\sigma,I}(A(S)) \ge \epsilon \, \middle| \, \sigma, I \right] \right] \; ,
$$
the inequality \eqref{equation:failure-probability} implies that there
exists $\sigma \in S_n$ and $i \in \{1,2,\dots,n\}$ such that under the
distribution $P_\sigma$, the algorithm $A$ fails to $\epsilon$-learn the target
$c_i$ with probability at least $\delta$.

To prove \eqref{equation:failure-probability}, we first introduce
necessary notation. Let $Y = (Y_1, Y_2, \dots, Y_m)$ of labels.
Let $M$ be $m \times n$ binary matrix $M$ with entries
$M_{i,j} = X_{i,\sigma(j)}$. Note that
$$
\Pr[M_{i,j} = 1] = p_j
$$
Note that the entries of $M$ are independent. Also, note that $M$ and $\sigma$
are independent. Let $C_1, C_2, \dots, C_n$ be the columns of $M$. Let $J = \{ j ~|~
1 \le j \le n, \ C_j = Y \}$ be the set of columns matching column of labels
$Y$. Note that, with probability one, $C_1 = Y$ and hence $1 \in J$.
Let $W(J) = \sum_{i \in J} p_i$ be the probability ``weight'' of columns in $J$.

For any set $S_\pi(J) = \{ \pi \in S_n ~:~ \forall i \in \{1,2,\dots,n\}
\setminus J, \pi(i) = i \}$ be the set of permutations of $\{1,2,\dots,n\}$ that
permutes elements of $J$ but keeps the remaining columns in place. Let $\alpha$
be randomly chosen from $S_n(J)$.

Note that $\sigma$ and $\alpha$ are independent. Somewhat less obviously,
$\sigma \circ \alpha$ and $M$ are independent and thus $(M,\sigma)$ and $(M,
\sigma \circ \alpha)$ have the same distribution; this is due to the fact that
conditioned on $M$ and $\alpha$, both $\sigma$ and $\sigma \circ \alpha$ have uniform
distribution over $S_n$. Futhermore, $X_{i,\sigma(j)} = X_{i,\sigma(\alpha(j))} =
M_{i,j}$ for all $i,j$, since $\alpha$ permutes only identical columns of $M$.
Hence, $(S,\sigma)$ and $(S,\sigma \circ \alpha)$ have the same joint
distribution. Therefore,
\begin{align*}
& \Pr \left[ \err_{\sigma,I}(A(S)) \ge \epsilon \right] \\
& \ge \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, b}}(A(S)) \ge \frac{1}{4} \right] \\
& = \frac{1}{2} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, b}}(A(S)) \ge \frac{1}{4} \right] + \frac{1}{2} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, b}}(A(S)) \ge \frac{1}{4} \right]  \\
& = \frac{1}{4} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, b}}(A(S)) \ge \frac{1}{4} \right] + \frac{1}{4} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, b}}(A(S)) \ge \frac{1}{4} \right] \\
& \qquad + \frac{1}{4} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, b}}(A(S)) \ge \frac{1}{4} \right] + \frac{1}{4} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, b}}(A(S)) \ge \frac{1}{4} \right] \\
& \ge \frac{1}{4} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, b}}(A(S)) \ge \frac{1}{4} \right] + \frac{1}{4} p_n^m  \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, a}}(A(S)) \ge \frac{1}{4} \right] \\
& \qquad + \frac{1}{4} \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, b}}(A(S)) \ge \frac{1}{4} \right] + \frac{1}{4} p_n^m \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, a}}(A(S)) \ge \frac{1}{4} \right] \\
& \ge \frac{1}{4} p_n^m \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, b}}(A(S)) \ge \frac{1}{4} \right] + \frac{1}{4} p_n^m  \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(1)},P_{\sigma, a}}(A(S)) \ge \frac{1}{4} \right] \\
& \qquad + \frac{1}{4} p_n^m \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, b}}(A(S)) \ge \frac{1}{4} \right] + \frac{1}{4} p_n^m \Pr \left[ |J| \ge 2 \wedge \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, a}}(A(S)) \ge \frac{1}{4} \right] \\
& \ge \frac{1}{4} p_n^m \Pr \left[ |J| \ge 2 \wedge \left( \err_{c_{\sigma(1)},P_{\sigma, b}}(A(S)) \ge \frac{1}{4} \vee \err_{c_{\sigma(1)},P_{\sigma, a}}(A(S)) \ge \frac{1}{4} \right. \right. \\
& \qquad \qquad \qquad \left. \left. \vee \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, b}}(A(S)) \ge \frac{1}{4} \vee \err_{c_{\sigma(r)},P_{\sigma \circ \alpha, a}}(A(S)) \ge \frac{1}{4} \right) \right] \\
& = \frac{1}{4} p_n^m \Pr \left[ |J| \ge 2 \right] \; .
\end{align*}
The last equality follows from Lemma~\ref{lemma:projection-distances}.

It remains to show that $\frac{1}{4} p_n^m \Pr \left[ |J| \ge 2 \right] \ge \delta$.
We first show that
$$
\Pr \left[ |J| \ge 2 \right] \ge \frac{1}{2} \; .
$$
Recall that $J$ is the subset of columns $C_1, C_2, \dots, C_n$ that are equal
to $C_1$. Let $x \in \{0,1\}^m$ be any binary vector. Conditioned on the event
$C_1 = x$, the events $C_2 = x$, $C_3 = x$, \dots, $C_n = x$ are independent.
Futhermore, $\Pr[C_j = x | C_1 = x] \ge p_n^m$.
$$
\Pr \left[ \bigwedge_{j=2}^n \{C_j \neq x\} ~\middle|~ C_1 = x \right] = \prod_{j=2}^n \Pr[C_j \neq x | C_1 = x] \le (1 - p_n^m)^{n-1} \le (1 - p_n^m)^n \; .
$$
Summing over all possible choices of $x$, we get
$$
\Pr[|J| \le 1] = \Pr \left[ \bigwedge_{j=2}^n \{C_j \neq C_1\} \right] \le (1 - p_n^m)^n \le e^{-n p_n^m} \; .
$$
To prove that $e^{-n p_n^m} \le \frac{1}{2}$, we show the equivalent statement $\ln \ln e^{n p_n^m} \ge \ln \ln 2$.
Indeed,
$$
\ln \ln e^{n p_n^m} = \ln n + m \ln p_n = \ln n + m \ln \log_2(3+n)
$$
\end{proof}

\section{Junk}

A permutation $\pi \in S_n$ is said to be a swap of $i,j$
if $\pi(i) = j$ and $\pi(j) = i$ and for all $k \in \{1,2,\dots,n\} \setminus \{i,j\}$
$\pi(k) = k$.

\begin{lemma}
\label{lemma:projection-distances}
Let $i \in \{2,3,\dots,n\}$. Let $\alpha \in S_n$ be a swap of $1$ and $i$.
Let $a,b \in \{0,1\}^n$ be binary vectors such that $a_i \neq b_i$
and $a_j = b_j$ for all $j \in \{1,2,\dots,n\} \setminus \{i\}$.
Let $h:\{0,1\}^n \to \{0,1\}$ be an arbitary function. Then, for any permutation $\sigma \in S_n$,
$$
\err_{c_{\sigma(1)},P_{\sigma,a}}(h) + \err_{c_{\sigma(1)},P_{\sigma,b}}(h) +
\err_{c_{\sigma(\alpha(1))},P_{\sigma \circ \alpha,b}}(h) + \err_{c_{\sigma(\alpha(1))},P_{\sigma \circ \alpha,b}}(h) \ge 1 \; .
$$
\end{lemma}

\begin{proof}
We claim that for any $x \in \{0,1\}^n$,
\begin{equation}
\label{equation:probability-ratio}
\frac{\displaystyle
\min \left\{ \Pr_{X \sim P_{\sigma,a}}[X = x] + \Pr_{X \sim P_{\sigma,b}}[X = x], \ \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X = x] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X = x] \right\}}{
\displaystyle \Pr_{X \sim P_{\sigma,a}}[X = x] + \Pr_{X \sim P_{\sigma,b}}[X = x] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X = x] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X = x]}
= \frac{1}{2} \; .
\end{equation}
Since $P_{\sigma,a}$,
$P_{\sigma,b}$, $P_{\sigma \circ \alpha,a}$, $P_{\sigma \circ \alpha,b}$
are product distributions and the marginal distributions of $X_{\sigma(k)}$
for any $k \not \in \{1,i\}$ under these four distributions are the same,
after cancelling common terms, we can write the fraction on the left side as $A/B$ where
\begin{multline*}
A =
\min \left\{ \Pr_{X \sim P_{\sigma,a}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}]
+ \Pr_{X \sim P_{\sigma,b}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}],  \right. \\
\left. \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}]
+ \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}]  \right\}
\end{multline*}
and
\begin{multline*}
B =
\Pr_{X \sim P_{\sigma,a}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] + \Pr_{X \sim P_{\sigma,b}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] \\
+ \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] \; .
\end{multline*}
Since $a_i \neq b_i$,
$$
\Pr_{X \sim P_{\sigma,a}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] + \Pr_{X \sim P_{\sigma,b}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] =
\Pr_{X \sim P_{\sigma,a}}[X_{\sigma(1)} = x_{\sigma(1)}] = \frac{1}{2}
$$
and
$$
\Pr_{X \sim P_{\sigma \circ \alpha,a}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X_{\sigma(1)} = x_{\sigma(1)} \wedge X_{\sigma(i)} = x_{\sigma(i)}] =
\Pr_{X \sim P_{\sigma \circ \alpha,a}}[X_{\sigma(i)} = x_{\sigma(i)}] = \frac{1}{2} \; .
$$

\allowdisplaybreaks
Equipped with \eqref{equation:probability-ratio} we finish the proof of the lemma:
\begin{align*}
& \err_{c_{\sigma(1)},P_{\sigma,a}}(h) + \err_{c_{\sigma(1)},P_{\sigma,b}}(h) + \err_{c_{\sigma(\alpha(1))},P_{\sigma \circ \alpha,b}}(h) + \err_{c_{\sigma(\alpha(1))},P_{\sigma \circ \alpha,b}}(h) \\
& = \err_{c_{\sigma(1)},P_{\sigma,a}}(h) + \err_{c_{\sigma(1)},P_{\sigma,b}}(h) + \err_{c_{\sigma(i)},P_{\sigma \circ \alpha,a}}(h) + \err_{c_{\sigma(i)},P_{\sigma \circ \alpha,b}}(h) \\
& = \Pr_{X \sim P_{\sigma,a}}[h(X) \neq X_{\sigma(1)}] + \Pr_{X \sim P_{\sigma,b}}[h(X) \neq X_{\sigma(1)}] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[h(X) \neq X_{\sigma(i)}] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[h(X) \neq X_{\sigma(i)}] \\
& = \sum_{x \in \{0,1\}^n} \Pr_{X \sim P_{\sigma,a}}[X = x \wedge h(x) \neq x_{\sigma(1)}] + \Pr_{X \sim P_{\sigma,b}}[X = x \wedge h(x) \neq x_{\sigma(1)}] \\
  & \qquad \qquad + \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X = x \wedge h(x) \neq x_{\sigma(i)}] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X = x \wedge h(x) \neq x_{\sigma(i)}] \\
& \ge \sum_{\substack{x \in \{0,1\}^n \\ x_{\sigma(1)} \neq x_{\sigma(i)}}} \Pr_{X \sim P_{\sigma,a}}[X = x \wedge h(x) \neq x_{\sigma(1)}] + \Pr_{X \sim P_{\sigma,b}}[X = x \wedge h(x) \neq x_{\sigma(1)}] \\
& \qquad \qquad + \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X = x \wedge h(x) \neq x_{\sigma(i)}] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X = x \wedge h(x) \neq x_{\sigma(i)}]  \\
& \ge \sum_{\substack{x \in \{0,1\}^n \\ x_{\sigma(1)} \neq x_{\sigma(i)}}} \min \left\{ \Pr_{X \sim P_{\sigma,a}}[X = x] + \Pr_{X \sim P_{\sigma,b}}[X = x], \ \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X = x] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X = x] \right\}
  \displaybreak[4] \\
& = \sum_{\substack{x \in \{0,1\}^n \\ x_{\sigma(1)} \neq x_{\sigma(i)}}} \frac{1}{2} \left(
\Pr_{X \sim P_{\sigma,a}}[X = x] + \Pr_{X \sim P_{\sigma,b}}[X = x] + \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X = x] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X = x] \right) \\
& = \frac{1}{2} \left( \Pr_{X \sim P_{\sigma,a}}[X_{\sigma(1)} \neq X_{\sigma(i)}] + \Pr_{X \sim P_{\sigma,b}}[X_{\sigma(1)} \neq X_{\sigma(i)}]
+ \Pr_{X \sim P_{\sigma \circ \alpha,a}}[X_{\sigma(1)} \neq X_{\sigma(i)}] + \Pr_{X \sim P_{\sigma \circ \alpha,b}}[X_{\sigma(1)} \neq X_{\sigma(i)}] \right) \\
& = \frac{1}{2} \left( \frac{1}{2} + \frac{1}{2} + \frac{1}{2} + \frac{1}{2} \right) = 1 \; .
\end{align*}
The first inequality is trivial. The second inequality follows from the fact that
if $x_{\sigma(1)} \neq x_{\sigma(i)}$ then $h(x)$ must be equal to either $x_{\sigma(1)}$ or $x_{\sigma(i)}$.
\end{proof}



\section{Realizable Case: Monotone Disjuctions}

Consider the class $C_n$ of monotone disjuctions over $D_n = \{0,1\}^n$.
There are $2^n$ functions in $C_n$. For each subset $I \subseteq \{1,2,\dots,n\}$
there is monotone disjuction $c_I:D_n \to \{0,1\}$ defined by
$$
c_I(x) = \bigvee_{i \in I} x_i \; .
$$
We define $c_\emptyset$ to be constant zero function.

We consider the same family of distributions $\P_n$ as in the previous section.
Let $P = P_{Identity} \in \P_n$. We compute the distance $d_P(c_I, c_J)$
between any two monotone disjuctions. For any subset
$K \subseteq \{1,2,\dots,n\}$, let $A_K$ be the event that all for all $i \in K$, $x_i = 0$.
\begin{align*}
d_P(c_I, c_J)
& = \Pr_{x \sim P}[c_I(x) \neq c_J(x)] \\
& = \Pr_{x \sim P}[A_{I \cap J} \wedge ((A_{I \setminus J} \wedge \overline{A_{J \setminus I}}) \vee (\overline{A_{I \setminus J}} \wedge A_{J \setminus I} )) ] \\
& = \Pr[A_{I \cap J}] \cdot \left\{ \Pr[A_{I \setminus J}] (1 - \Pr[A_{J \setminus I}]) + (1 - \Pr[A_{I \setminus J}]) \Pr[A_{J \setminus I}] \right\} \\
& = \Pr[A_{I \cap J}] \cdot \left\{ \Pr[A_{I \setminus J}] + \Pr[A_{J \setminus I}] - 2 \Pr[A_{I \setminus J}] \Pr[A_{J \setminus I}] \right\} \\
& = \Pr[A_I] + \Pr[A_J] - 2 \Pr[A_{I \cup J}] \\
& = \prod_{i \in I} (1 - p_i) + \prod_{j \in J} (1 - p_j) - 2 \prod_{k \in I \cup J} (1 - p_k) \; . \\
\end{align*}

\section{Agnostic Model}

Let $C$ be a concept class over a domain $X$. Let $P$ be any distribution over
$X$. We define the ``disagreement'' pseudometric $d_P(c,c') = \Pr_{(x,y) \sim
P}[c(x) = c'(x)]$ on $C$. We denote by $N(C,P,\epsilon,)$ be the size of the
smallest $\epsilon$-cover of $C$ with respect to $d_P$.

Let $R(c) = \Pr_{(x,y) \sim P}[c(x) \neq y]$ be the risk.
For an i.i.d. sample $(X_1, Y_1), \dots, (X_n, Y_n)$ be an i.i.d. sample
from $P$, we define the empirical risk
$$
R_n(c) = \sum_{i=1}^n \indicator{c(X_i) \neq Y_i} \; .
$$
Finally, let $c_n = \argmin_{c \in C} R_n(c)$.

\bibliography{biblio}
\bibliographystyle{plainnat}

\end{document}
